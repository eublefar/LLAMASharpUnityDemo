<?xml version="1.0"?>
<doc>
    <assembly>
        <name>LLamaSharp</name>
    </assembly>
    <members>
        <member name="T:System.Runtime.CompilerServices.IsExternalInit">
            <summary>
                Reserved to be used by the compiler for tracking metadata.
                This class should not be used by developers in source code.
            </summary>
            <remarks>
                This definition is provided by the <i>IsExternalInit</i> NuGet package (https://www.nuget.org/packages/IsExternalInit).
                Please see https://github.com/manuelroemer/IsExternalInit for more information.
            </remarks>
        </member>
        <member name="T:LLama.Abstractions.IContextParams">
            <summary>
            The parameters for initializing a LLama context from a model.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.ContextSize">
            <summary>
            Model context size (n_ctx)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.BatchSize">
            <summary>
            batch size for prompt processing (must be >=32 to use BLAS) (n_batch)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.Seed">
            <summary>
            Seed for the random number generator (seed)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.EmbeddingMode">
            <summary>
            Whether to use embedding mode. (embedding) Note that if this is set to true, 
            The LLamaModel won't produce text response anymore.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.RopeFrequencyBase">
            <summary>
            RoPE base frequency (null to fetch from the model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.RopeFrequencyScale">
            <summary>
            RoPE frequency scaling factor (null to fetch from the model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.Encoding">
            <summary>
            The encoding to use for models
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.Threads">
            <summary>
            Number of threads (null = autodetect) (n_threads)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.BatchThreads">
            <summary>
            Number of threads to use for batch processing (null = autodetect) (n_threads)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.YarnExtrapolationFactor">
            <summary>
            YaRN extrapolation mix factor (null = from model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.YarnAttentionFactor">
            <summary>
            YaRN magnitude scaling factor (null = from model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.YarnBetaFast">
            <summary>
            YaRN low correction dim (null = from model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.YarnBetaSlow">
            <summary>
            YaRN high correction dim (null = from model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.YarnOriginalContext">
            <summary>
            YaRN original context length (null = from model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.YarnScalingType">
            <summary>
            YaRN scaling method to use.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.TypeK">
            <summary>
            Override the type of the K cache
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.TypeV">
            <summary>
            Override the type of the V cache
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IContextParams.NoKqvOffload">
            <summary>
            Whether to disable offloading the KQV cache to the GPU
            </summary>
        </member>
        <member name="T:LLama.Abstractions.IHistoryTransform">
            <summary>
            Transform history to plain text and vice versa.
            </summary>
        </member>
        <member name="M:LLama.Abstractions.IHistoryTransform.HistoryToText(LLama.Common.ChatHistory)">
            <summary>
            Convert a ChatHistory instance to plain text.
            </summary>
            <param name="history">The ChatHistory instance</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Abstractions.IHistoryTransform.TextToHistory(LLama.Common.AuthorRole,System.String)">
            <summary>
            Converts plain text to a ChatHistory instance.
            </summary>
            <param name="role">The role for the author.</param>
            <param name="text">The chat history as plain text.</param>
            <returns>The updated history.</returns>
        </member>
        <member name="T:LLama.Abstractions.IInferenceParams">
            <summary>
            The paramters used for inference.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.TokensKeep">
            <summary>
            number of tokens to keep from initial prompt
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.MaxTokens">
            <summary>
            how many new tokens to predict (n_predict), set to -1 to inifinitely generate response
            until it complete.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.LogitBias">
            <summary>
            logit bias for specific tokens
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.AntiPrompts">
            <summary>
            Sequences where the model will stop generating further tokens.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.TopK">
            <summary>
             0 or lower to use vocab size
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.TopP">
            <summary>llama_eval
            1.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.MinP">
            <summary>llama_eval
            0.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.TfsZ">
            <summary>
            1.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.TypicalP">
            <summary>
            1.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.Temperature">
            <summary>
            1.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.RepeatPenalty">
            <summary>
            1.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.RepeatLastTokensCount">
            <summary>
            last n tokens to penalize (0 = disable penalty, -1 = context size) (repeat_last_n)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.FrequencyPenalty">
            <summary>
            frequency penalty coefficient
            0.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.PresencePenalty">
            <summary>
            presence penalty coefficient
            0.0 = disabled
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.Mirostat">
            <summary>
            Mirostat uses tokens instead of words.
            algorithm described in the paper https://arxiv.org/abs/2007.14966.
            0 = disabled, 1 = mirostat, 2 = mirostat 2.0
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.MirostatTau">
            <summary>
            target entropy
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.MirostatEta">
            <summary>
            learning rate
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.PenalizeNL">
            <summary>
            consider newlines as a repeatable token (penalize_nl)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.Grammar">
            <summary>
            Grammar to constrain possible tokens
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IInferenceParams.SamplingPipeline">
            <summary>
            Set a custom sampling pipeline to use. <b>If this is set All other sampling parameters are ignored!</b>
            </summary>
        </member>
        <member name="T:LLama.Abstractions.ILLamaExecutor">
            <summary>
            A high level interface for LLama models.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.ILLamaExecutor.Context">
            <summary>
            The loaded context for this executor.
            </summary>
        </member>
        <member name="M:LLama.Abstractions.ILLamaExecutor.InferAsync(System.String,LLama.Abstractions.IInferenceParams,System.Threading.CancellationToken)">
            <summary>
            Asynchronously infers a response from the model.
            </summary>
            <param name="text">Your prompt</param>
            <param name="inferenceParams">Any additional parameters</param>
            <param name="token">A cancellation token.</param>
            <returns></returns>
        </member>
        <member name="T:LLama.Abstractions.ILLamaParams">
            <summary>
            Convenience interface for implementing both type of parameters.
            </summary>
            <remarks>Mostly exists for backwards compatibility reasons, when these two were not split.</remarks>
        </member>
        <member name="T:LLama.Abstractions.IModelParams">
            <summary>
            The parameters for initializing a LLama model.
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.MainGpu">
            <summary>
            main_gpu interpretation depends on split_mode:
            <list type="bullet">
                <item>
                    <term>None</term>
                    <description>The GPU that is used for the entire mode.</description>
                </item>
                <item>
                    <term>Row</term>
                    <description>The GPU that is used for small tensors and intermediate results.</description>
                </item>
                <item>
                    <term>Layer</term>
                    <description>Ignored.</description>
                </item>
            </list>
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.SplitMode">
            <summary>
            How to split the model across multiple GPUs
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.GpuLayerCount">
            <summary>
            Number of layers to run in VRAM / GPU memory (n_gpu_layers)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.UseMemorymap">
            <summary>
            Use mmap for faster loads (use_mmap)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.UseMemoryLock">
            <summary>
            Use mlock to keep model in memory (use_mlock)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.ModelPath">
            <summary>
            Model path (model)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.TensorSplits">
            <summary>
            how split tensors should be distributed across GPUs
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.VocabOnly">
            <summary>
            Load vocab only (no weights)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.LoraAdapters">
            <summary>
            List of LoRA adapters to apply
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.LoraBase">
            <summary>
            base model path for the lora adapter (lora_base)
            </summary>
        </member>
        <member name="P:LLama.Abstractions.IModelParams.MetadataOverrides">
            <summary>
            Override specific metadata items in the model
            </summary>
        </member>
        <member name="T:LLama.Abstractions.LoraAdapter">
            <summary>
            A LoRA adapter to apply to a model
            </summary>
            <param name="Path">Path to the LoRA file</param>
            <param name="Scale">Strength of this LoRA</param>
        </member>
        <member name="M:LLama.Abstractions.LoraAdapter.#ctor(System.String,System.Single)">
            <summary>
            A LoRA adapter to apply to a model
            </summary>
            <param name="Path">Path to the LoRA file</param>
            <param name="Scale">Strength of this LoRA</param>
        </member>
        <member name="P:LLama.Abstractions.LoraAdapter.Path">
            <summary>Path to the LoRA file</summary>
        </member>
        <member name="P:LLama.Abstractions.LoraAdapter.Scale">
            <summary>Strength of this LoRA</summary>
        </member>
        <member name="T:LLama.Abstractions.AdapterCollection">
            <summary>
            A list of LoraAdapter objects
            </summary>
        </member>
        <member name="M:LLama.Abstractions.AdapterCollection.Equals(LLama.Abstractions.AdapterCollection)">
            <inheritdoc />
        </member>
        <member name="M:LLama.Abstractions.AdapterCollection.Equals(System.Object)">
            <inheritdoc/>
        </member>
        <member name="M:LLama.Abstractions.AdapterCollection.GetHashCode">
            <inheritdoc/>
        </member>
        <member name="T:LLama.Abstractions.TensorSplitsCollection">
            <summary>
            A fixed size array to set the tensor splits across multiple GPUs
            </summary>
        </member>
        <member name="P:LLama.Abstractions.TensorSplitsCollection.Length">
            <summary>
            The size of this array
            </summary>
        </member>
        <member name="P:LLama.Abstractions.TensorSplitsCollection.Item(System.Int32)">
            <summary>
            Get or set the proportion of work to do on the given device.
            </summary>
            <remarks>"[ 3, 2 ]" will assign 60% of the data to GPU 0 and 40% to GPU 1.</remarks>
            <param name="index"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Abstractions.TensorSplitsCollection.#ctor(System.Single[])">
            <summary>
            Create a new tensor splits collection, copying the given values
            </summary>
            <param name="splits"></param>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.Abstractions.TensorSplitsCollection.#ctor">
            <summary>
            Create a new tensor splits collection with all values initialised to the default
            </summary>
        </member>
        <member name="M:LLama.Abstractions.TensorSplitsCollection.Clear">
            <summary>
            Set all values to zero
            </summary>
        </member>
        <member name="M:LLama.Abstractions.TensorSplitsCollection.GetEnumerator">
            <inheritdoc />
        </member>
        <member name="M:LLama.Abstractions.TensorSplitsCollection.System#Collections#IEnumerable#GetEnumerator">
            <inheritdoc />
        </member>
        <member name="T:LLama.Abstractions.TensorSplitsCollectionConverter">
            <summary>
            A JSON converter for <see cref="T:LLama.Abstractions.TensorSplitsCollection"/>
            </summary>
        </member>
        <member name="M:LLama.Abstractions.TensorSplitsCollectionConverter.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <inheritdoc/>
        </member>
        <member name="M:LLama.Abstractions.TensorSplitsCollectionConverter.Write(System.Text.Json.Utf8JsonWriter,LLama.Abstractions.TensorSplitsCollection,System.Text.Json.JsonSerializerOptions)">
            <inheritdoc/>
        </member>
        <member name="T:LLama.Abstractions.MetadataOverride">
            <summary>
            An override for a single key/value pair in model metadata
            </summary>
        </member>
        <member name="P:LLama.Abstractions.MetadataOverride.Key">
            <summary>
            Get the key being overriden by this override
            </summary>
        </member>
        <member name="M:LLama.Abstractions.MetadataOverride.#ctor(System.String,System.Int32)">
            <summary>
            Create a new override for an int key
            </summary>
            <param name="key"></param>
            <param name="value"></param>
        </member>
        <member name="M:LLama.Abstractions.MetadataOverride.#ctor(System.String,System.Single)">
            <summary>
            Create a new override for a float key
            </summary>
            <param name="key"></param>
            <param name="value"></param>
        </member>
        <member name="M:LLama.Abstractions.MetadataOverride.#ctor(System.String,System.Boolean)">
            <summary>
            Create a new override for a boolean key
            </summary>
            <param name="key"></param>
            <param name="value"></param>
        </member>
        <member name="T:LLama.Abstractions.MetadataOverrideConverter">
            <summary>
            A JSON converter for <see cref="T:LLama.Abstractions.MetadataOverride"/>
            </summary>
        </member>
        <member name="M:LLama.Abstractions.MetadataOverrideConverter.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <inheritdoc/>
        </member>
        <member name="M:LLama.Abstractions.MetadataOverrideConverter.Write(System.Text.Json.Utf8JsonWriter,LLama.Abstractions.MetadataOverride,System.Text.Json.JsonSerializerOptions)">
            <inheritdoc/>
        </member>
        <member name="T:LLama.Abstractions.ITextStreamTransform">
            <summary>
            Takes a stream of tokens and transforms them.
            </summary>
        </member>
        <member name="M:LLama.Abstractions.ITextStreamTransform.TransformAsync(System.Collections.Generic.IAsyncEnumerable{System.String})">
            <summary>
            Takes a stream of tokens and transforms them, returning a new stream of tokens asynchronously.
            </summary>
            <param name="tokens"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.Abstractions.ITextTransform">
            <summary>
            An interface for text transformations.
            These can be used to compose a pipeline of text transformations, such as:
            - Tokenization
            - Lowercasing
            - Punctuation removal
            - Trimming
            - etc.
            </summary>
        </member>
        <member name="M:LLama.Abstractions.ITextTransform.Transform(System.String)">
            <summary>
            Takes a string and transforms it.
            </summary>
            <param name="text"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.AntipromptProcessor">
            <summary>
            AntipromptProcessor keeps track of past tokens looking for any set Anti-Prompts
            </summary>
        </member>
        <member name="M:LLama.AntipromptProcessor.#ctor(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Initializes a new instance of the <see cref="T:LLama.AntipromptProcessor"/> class.
            </summary>
            <param name="antiprompts">The antiprompts.</param>
        </member>
        <member name="M:LLama.AntipromptProcessor.AddAntiprompt(System.String)">
            <summary>
            Add an antiprompt to the collection
            </summary>
            <param name="antiprompt"></param>
        </member>
        <member name="M:LLama.AntipromptProcessor.SetAntiprompts(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Overwrite all current antiprompts with a new set
            </summary>
            <param name="antiprompts"></param>
        </member>
        <member name="M:LLama.AntipromptProcessor.Add(System.String)">
            <summary>
            Add some text and check if the buffer now ends with any antiprompt
            </summary>
            <param name="text"></param>
            <returns>true if the text buffer ends with any antiprompt</returns>
        </member>
        <member name="T:LLama.Batched.BatchedExecutor">
            <summary>
            A batched executor that can infer multiple separate "conversations" simultaneously.
            </summary>
        </member>
        <member name="P:LLama.Batched.BatchedExecutor.Epoch">
            <summary>
            Epoch is incremented every time Infer is called. Conversations can use this to keep track of
            whether they're waiting for inference, or can be sampled.
            </summary>
        </member>
        <member name="P:LLama.Batched.BatchedExecutor.Context">
            <summary>
            The <see cref="T:LLama.LLamaContext"/> this executor is using
            </summary>
        </member>
        <member name="P:LLama.Batched.BatchedExecutor.Model">
            <summary>
            The <see cref="T:LLama.LLamaWeights"/> this executor is using
            </summary>
        </member>
        <member name="P:LLama.Batched.BatchedExecutor.BatchedTokenCount">
            <summary>
            Get the number of tokens in the batch, waiting for <see cref="M:LLama.Batched.BatchedExecutor.Infer(System.Threading.CancellationToken)"/> to be called
            </summary>
        </member>
        <member name="P:LLama.Batched.BatchedExecutor.IsDisposed">
            <summary>
            Check if this executor has been disposed.
            </summary>
        </member>
        <member name="M:LLama.Batched.BatchedExecutor.#ctor(LLama.LLamaWeights,LLama.Abstractions.IContextParams)">
            <summary>
            Create a new batched executor
            </summary>
            <param name="model">The model to use</param>
            <param name="contextParams">Parameters to create a new context</param>
        </member>
        <member name="M:LLama.Batched.BatchedExecutor.Prompt(System.String)">
            <summary>
            Start a new <see cref="T:LLama.Batched.Conversation"/> with the given prompt
            </summary>
            <param name="prompt"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Batched.BatchedExecutor.Infer(System.Threading.CancellationToken)">
             <summary>
             Run inference for all conversations in the batch which have pending tokens.
            
             If the result is `NoKvSlot` then there is not enough memory for inference, try disposing some conversation
             threads and running inference again.
             </summary>
        </member>
        <member name="M:LLama.Batched.BatchedExecutor.Dispose">
            <inheritdoc />
        </member>
        <member name="T:LLama.Batched.Conversation">
            <summary>
            A single conversation thread that can be prompted (adding tokens from the user) or inferred (extracting a token from the LLM)
            </summary>
        </member>
        <member name="P:LLama.Batched.Conversation.Executor">
            <summary>
            The executor which this conversation belongs to
            </summary>
        </member>
        <member name="P:LLama.Batched.Conversation.ConversationId">
            <summary>
            Unique ID for this conversation
            </summary>
        </member>
        <member name="P:LLama.Batched.Conversation.TokenCount">
            <summary>
            Total number of tokens in this conversation, cannot exceed the context length.
            </summary>
        </member>
        <member name="P:LLama.Batched.Conversation.IsDisposed">
            <summary>
            Indicates if this conversation has been disposed, nothing can be done with a disposed conversation
            </summary>
        </member>
        <member name="P:LLama.Batched.Conversation.RequiresInference">
            <summary>
            Indicates if this conversation is waiting for inference to be run on the executor. "Prompt" and "Sample" cannot be called when this is true.
            </summary>
        </member>
        <member name="P:LLama.Batched.Conversation.RequiresSampling">
            <summary>
            Indicates that this conversation should be sampled.
            </summary>
        </member>
        <member name="M:LLama.Batched.Conversation.Dispose">
            <summary>
            End this conversation, freeing all resources used by it
            </summary>
            <exception cref="T:System.ObjectDisposedException"></exception>
        </member>
        <member name="M:LLama.Batched.Conversation.Fork">
            <summary>
            Create a copy of the current conversation
            </summary>
            <remarks>The copy shares internal state, so consumes very little extra memory.</remarks>
            <returns></returns>
            <exception cref="T:System.ObjectDisposedException"></exception>
        </member>
        <member name="M:LLama.Batched.Conversation.Sample">
            <summary>
            Get the logits from this conversation, ready for sampling
            </summary>
            <returns></returns>
            <exception cref="T:System.ObjectDisposedException"></exception>
            <exception cref="T:LLama.Batched.CannotSampleRequiresPromptException">Thrown if this conversation was not prompted before the previous call to infer</exception>
            <exception cref="T:LLama.Batched.CannotSampleRequiresInferenceException">Thrown if Infer() must be called on the executor</exception>
        </member>
        <member name="M:LLama.Batched.Conversation.Prompt(System.String)">
            <summary>
            Add tokens to this conversation
            </summary>
            <param name="input"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Batched.Conversation.Prompt(System.Collections.Generic.IReadOnlyList{LLama.Native.LLamaToken})">
            <summary>
            Add tokens to this conversation
            </summary>
            <param name="tokens"></param>
            <returns></returns>
            <exception cref="T:System.ObjectDisposedException"></exception>
        </member>
        <member name="M:LLama.Batched.Conversation.Prompt(LLama.Native.LLamaToken)">
            <summary>
            Add a single token to this conversation
            </summary>
            <param name="token"></param>
            <returns></returns>
            <exception cref="T:System.ObjectDisposedException"></exception>
            <exception cref="T:System.InvalidOperationException"></exception>
        </member>
        <member name="M:LLama.Batched.Conversation.Modify(LLama.Batched.Conversation.ModifyKvCache)">
            <summary>
            Directly modify the KV cache of this conversation
            </summary>
            <param name="modifier"></param>
            <exception cref="T:LLama.Batched.CannotModifyWhileRequiresInference">Thrown if this method is called while <see cref="P:LLama.Batched.Conversation.RequiresInference"/> == true</exception>
        </member>
        <member name="T:LLama.Batched.Conversation.KvAccessor">
            <summary>
            Provides direct access to the KV cache of a <see cref="T:LLama.Batched.Conversation"/>.
            See <see cref="M:LLama.Batched.Conversation.Modify(LLama.Batched.Conversation.ModifyKvCache)"/> for how to use this.
            </summary>
        </member>
        <member name="M:LLama.Batched.Conversation.KvAccessor.Remove(LLama.Native.LLamaPos,LLama.Native.LLamaPos)">
            <summary>
            Removes all tokens that have positions in [start, end)
            </summary>
            <param name="start">Start position (inclusive)</param>
            <param name="end">End position (exclusive)</param>
        </member>
        <member name="M:LLama.Batched.Conversation.KvAccessor.Remove(LLama.Native.LLamaPos,System.Int32)">
            <summary>
            Removes all tokens starting from the given position
            </summary>
            <param name="start">Start position (inclusive)</param>
            <param name="count">Number of tokens</param>
        </member>
        <member name="M:LLama.Batched.Conversation.KvAccessor.Shift(LLama.Native.LLamaPos,LLama.Native.LLamaPos,System.Int32)">
            <summary>
            Adds relative position "delta" to all tokens that have positions in [p0, p1).
            If the KV cache is RoPEd, the KV data is updated
            accordingly
            </summary>
            <param name="start">Start position (inclusive)</param>
            <param name="end">End position (exclusive)</param>
            <param name="delta">Amount to add on to each token position</param>
        </member>
        <member name="M:LLama.Batched.Conversation.KvAccessor.Divide(LLama.Native.LLamaPos,LLama.Native.LLamaPos,System.Int32)">
            <summary>
            Integer division of the positions by factor of `d > 1`.
            If the KV cache is RoPEd, the KV data is updated accordingly.
            </summary>
            <param name="start">Start position (inclusive). If less than zero, it is clamped to zero.</param>
            <param name="end">End position (exclusive). If less than zero, it is treated as "infinity".</param>
            <param name="divisor">Amount to divide each position by.</param>
        </member>
        <member name="T:LLama.Batched.Conversation.ModifyKvCache">
            <summary>
            A function which can temporarily access the KV cache of a <see cref="T:LLama.Batched.Conversation"/> to modify it directly
            </summary>
            <param name="end">The current end token of this conversation</param>
            <param name="kv">An <see cref="T:LLama.Batched.Conversation.KvAccessor"/> which allows direct access to modify the KV cache</param>
            <returns>The new end token position</returns>
        </member>
        <member name="T:LLama.Batched.ConversationExtensions">
            <summary>
            Extension method for <see cref="T:LLama.Batched.Conversation"/>
            </summary>
        </member>
        <member name="M:LLama.Batched.ConversationExtensions.Rewind(LLama.Batched.Conversation,System.Int32)">
            <summary>
            Rewind a <see cref="T:LLama.Batched.Conversation"/> back to an earlier state by removing tokens from the end
            </summary>
            <param name="conversation">The conversation to rewind</param>
            <param name="tokens">The number of tokens to rewind</param>
            <exception cref="T:System.ArgumentOutOfRangeException">Thrown if `tokens` parameter is larger than TokenCount</exception>
        </member>
        <member name="M:LLama.Batched.ConversationExtensions.ShiftLeft(LLama.Batched.Conversation,System.Int32,System.Int32)">
            <summary>
            Shift all tokens over to the left, removing "count" tokens from the start and shifting everything over.
            Leaves "keep" tokens at the start completely untouched. This can be used to free up space when the context
            gets full, keeping the prompt at the start intact.
            </summary>
            <param name="conversation">The conversation to rewind</param>
            <param name="count">How much to shift tokens over by</param>
            <param name="keep">The number of tokens at the start which should <b>not</b> be shifted</param>
        </member>
        <member name="T:LLama.Batched.ExperimentalBatchedExecutorException">
            <summary>
            Base class for exceptions thrown from <see cref="T:LLama.Batched.BatchedExecutor"/>
            </summary>
        </member>
        <member name="T:LLama.Batched.AlreadyPromptedConversationException">
            <summary>
            This exception is thrown when "Prompt()" is called on a <see cref="T:LLama.Batched.Conversation"/> which has
            already been prompted and before "Infer()" has been called on the associated
            <see cref="T:LLama.Batched.BatchedExecutor"/>.
            </summary>
        </member>
        <member name="T:LLama.Batched.CannotSampleRequiresInferenceException">
            <summary>
            This exception is thrown when "Sample()" is called on a <see cref="T:LLama.Batched.Conversation"/> which has
            already been prompted and before "Infer()" has been called on the associated
            <see cref="T:LLama.Batched.BatchedExecutor"/>.
            </summary>
        </member>
        <member name="T:LLama.Batched.CannotSampleRequiresPromptException">
            <summary>
            This exception is thrown when "Sample()" is called on a <see cref="T:LLama.Batched.Conversation"/> which was not
            first prompted.
            <see cref="T:LLama.Batched.BatchedExecutor"/>.
            </summary>
        </member>
        <member name="T:LLama.Batched.CannotForkWhileRequiresInference">
            <summary>
            This exception is thrown when <see cref="M:LLama.Batched.Conversation.Fork"/> is called when <see cref="P:LLama.Batched.Conversation.RequiresInference"/> = true
            </summary>
        </member>
        <member name="T:LLama.Batched.CannotModifyWhileRequiresInference">
            <summary>
            This exception is thrown when <see cref="M:LLama.Batched.Conversation.Modify(LLama.Batched.Conversation.ModifyKvCache)"/> is called when <see cref="P:LLama.Batched.Conversation.RequiresInference"/> = true
            </summary>
        </member>
        <member name="T:LLama.ChatSession">
            <summary>
            The main chat session class.
            </summary>
        </member>
        <member name="P:LLama.ChatSession.Executor">
            <summary>
            The executor for this session.
            </summary>
        </member>
        <member name="P:LLama.ChatSession.History">
            <summary>
            The chat history for this session.
            </summary>
        </member>
        <member name="P:LLama.ChatSession.HistoryTransform">
            <summary>
            The history transform used in this session.
            </summary>
        </member>
        <member name="P:LLama.ChatSession.InputTransformPipeline">
            <summary>
            The input transform pipeline used in this session.
            </summary>
        </member>
        <member name="F:LLama.ChatSession.OutputTransform">
            <summary>
            The output transform used in this session.
            </summary>
        </member>
        <member name="M:LLama.ChatSession.#ctor(LLama.Abstractions.ILLamaExecutor)">
            <summary>
            Create a new chat session.
            </summary>
            <param name="executor">The executor for this session</param>
        </member>
        <member name="M:LLama.ChatSession.#ctor(LLama.Abstractions.ILLamaExecutor,LLama.Common.ChatHistory)">
            <summary>
            Create a new chat session with a custom history.
            </summary>
            <param name="executor"></param>
            <param name="history"></param>
        </member>
        <member name="M:LLama.ChatSession.WithHistoryTransform(LLama.Abstractions.IHistoryTransform)">
            <summary>
            Use a custom history transform.
            </summary>
            <param name="transform"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.AddInputTransform(LLama.Abstractions.ITextTransform)">
            <summary>
            Add a text transform to the input transform pipeline.
            </summary>
            <param name="transform"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.WithOutputTransform(LLama.Abstractions.ITextStreamTransform)">
            <summary>
            Use a custom output transform.
            </summary>
            <param name="transform"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.SaveSession(System.String)">
            <summary>
            Save a session from a directory.
            </summary>
            <param name="path"></param>
            <returns></returns>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.ChatSession.LoadSession(System.String)">
            <summary>
            Load a session from a directory.
            </summary>
            <param name="path"></param>
            <returns></returns>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.ChatSession.AddMessage(LLama.Common.ChatHistory.Message)">
            <summary>
            Add a message to the chat history.
            </summary>
            <param name="message"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.AddSystemMessage(System.String)">
            <summary>
            Add a system message to the chat history.
            </summary>
            <param name="content"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.AddAssistantMessage(System.String)">
            <summary>
            Add an assistant message to the chat history.
            </summary>
            <param name="content"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.AddUserMessage(System.String)">
            <summary>
            Add a user message to the chat history.
            </summary>
            <param name="content"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.RemoveLastMessage">
            <summary>
            Remove the last message from the chat history.
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.ReplaceUserMessage(LLama.Common.ChatHistory.Message,LLama.Common.ChatHistory.Message)">
            <summary>
            Replace a user message with a new message and remove all messages after the new message.
            This is useful when the user wants to edit a message. And regenerate the response.
            </summary>
            <param name="oldMessage"></param>
            <param name="newMessage"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.ChatAsync(LLama.Common.ChatHistory.Message,System.Boolean,LLama.Abstractions.IInferenceParams,System.Threading.CancellationToken)">
            <summary>
            Chat with the model.
            </summary>
            <param name="message"></param>
            <param name="inferenceParams"></param>
            <param name="applyInputTransformPipeline"></param>
            <param name="cancellationToken"></param>
            <returns></returns>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.ChatSession.ChatAsync(LLama.Common.ChatHistory.Message,LLama.Abstractions.IInferenceParams,System.Threading.CancellationToken)">
            <summary>
            Chat with the model.
            </summary>
            <param name="message"></param>
            <param name="inferenceParams"></param>
            <param name="cancellationToken"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.ChatAsync(LLama.Common.ChatHistory,System.Boolean,LLama.Abstractions.IInferenceParams,System.Threading.CancellationToken)">
            <summary>
            Chat with the model.
            </summary>
            <param name="history"></param>
            <param name="applyInputTransformPipeline"></param>
            <param name="inferenceParams"></param>
            <param name="cancellationToken"></param>
            <returns></returns>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.ChatSession.ChatAsync(LLama.Common.ChatHistory,LLama.Abstractions.IInferenceParams,System.Threading.CancellationToken)">
            <summary>
            Chat with the model.
            </summary>
            <param name="history"></param>
            <param name="inferenceParams"></param>
            <param name="cancellationToken"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.ChatSession.RegenerateAssistantMessageAsync(LLama.Common.InferenceParams,System.Threading.CancellationToken)">
            <summary>
            Regenerate the last assistant message.
            </summary>
            <param name="inferenceParams"></param>
            <param name="cancellationToken"></param>
            <returns></returns>
            <exception cref="T:System.InvalidOperationException"></exception>
        </member>
        <member name="T:LLama.Common.AuthorRole">
            <summary>
            Role of the message author, e.g. user/assistant/system
            </summary>
        </member>
        <member name="F:LLama.Common.AuthorRole.Unknown">
            <summary>
            Role is unknown
            </summary>
        </member>
        <member name="F:LLama.Common.AuthorRole.System">
            <summary>
            Message comes from a "system" prompt, not written by a user or language model
            </summary>
        </member>
        <member name="F:LLama.Common.AuthorRole.User">
            <summary>
            Message comes from the user
            </summary>
        </member>
        <member name="F:LLama.Common.AuthorRole.Assistant">
            <summary>
            Messages was generated by the language model
            </summary>
        </member>
        <member name="T:LLama.Common.ChatHistory">
            <summary>
            The chat history class
            </summary>
        </member>
        <member name="T:LLama.Common.ChatHistory.Message">
            <summary>
            Chat message representation
            </summary>
        </member>
        <member name="P:LLama.Common.ChatHistory.Message.AuthorRole">
            <summary>
            Role of the message author, e.g. user/assistant/system
            </summary>
        </member>
        <member name="P:LLama.Common.ChatHistory.Message.Content">
            <summary>
            Message content
            </summary>
        </member>
        <member name="M:LLama.Common.ChatHistory.Message.#ctor(LLama.Common.AuthorRole,System.String)">
            <summary>
            Create a new instance
            </summary>
            <param name="authorRole">Role of message author</param>
            <param name="content">Message content</param>
        </member>
        <member name="P:LLama.Common.ChatHistory.Messages">
            <summary>
            List of messages in the chat
            </summary>
        </member>
        <member name="M:LLama.Common.ChatHistory.#ctor">
            <summary>
            Create a new instance of the chat content class
            </summary>
        </member>
        <member name="M:LLama.Common.ChatHistory.AddMessage(LLama.Common.AuthorRole,System.String)">
            <summary>
            Add a message to the chat history
            </summary>
            <param name="authorRole">Role of the message author</param>
            <param name="content">Message content</param>
        </member>
        <member name="M:LLama.Common.ChatHistory.ToJson">
            <summary>
            Serialize the chat history to JSON
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Common.ChatHistory.FromJson(System.String)">
            <summary>
            Deserialize a chat history from JSON
            </summary>
            <param name="json"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.Common.FixedSizeQueue`1">
            <summary>
            A queue with fixed storage size.
            Currently it's only a naive implementation and needs to be further optimized in the future.
            </summary>
        </member>
        <member name="P:LLama.Common.FixedSizeQueue`1.Item(System.Int32)">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.FixedSizeQueue`1.Count">
            <summary>
            Number of items in this queue
            </summary>
        </member>
        <member name="P:LLama.Common.FixedSizeQueue`1.Capacity">
            <summary>
            Maximum number of items allowed in this queue
            </summary>
        </member>
        <member name="M:LLama.Common.FixedSizeQueue`1.#ctor(System.Int32)">
            <summary>
            Create a new queue
            </summary>
            <param name="size">the maximum number of items to store in this queue</param>
        </member>
        <member name="M:LLama.Common.FixedSizeQueue`1.#ctor(System.Int32,System.Collections.Generic.IEnumerable{`0})">
            <summary>
            Fill the quene with the data. Please ensure that data.Count &lt;= size
            </summary>
            <param name="size"></param>
            <param name="data"></param>
        </member>
        <member name="M:LLama.Common.FixedSizeQueue`1.Enqueue(`0)">
            <summary>
            Enquene an element.
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Common.FixedSizeQueue`1.GetEnumerator">
            <inheritdoc />
        </member>
        <member name="M:LLama.Common.FixedSizeQueue`1.System#Collections#IEnumerable#GetEnumerator">
            <inheritdoc />
        </member>
        <member name="T:LLama.Common.InferenceParams">
            <summary>
            The paramters used for inference.
            </summary>
        </member>
        <member name="P:LLama.Common.InferenceParams.TokensKeep">
            <summary>
            number of tokens to keep from initial prompt
            </summary>
        </member>
        <member name="P:LLama.Common.InferenceParams.MaxTokens">
            <summary>
            how many new tokens to predict (n_predict), set to -1 to inifinitely generate response
            until it complete.
            </summary>
        </member>
        <member name="P:LLama.Common.InferenceParams.LogitBias">
            <summary>
            logit bias for specific tokens
            </summary>
        </member>
        <member name="P:LLama.Common.InferenceParams.AntiPrompts">
            <summary>
            Sequences where the model will stop generating further tokens.
            </summary>
        </member>
        <member name="P:LLama.Common.InferenceParams.TopK">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.TopP">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.MinP">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.TfsZ">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.TypicalP">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.Temperature">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.RepeatPenalty">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.RepeatLastTokensCount">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.FrequencyPenalty">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.PresencePenalty">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.Mirostat">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.MirostatTau">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.MirostatEta">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.PenalizeNL">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.Grammar">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.InferenceParams.SamplingPipeline">
            <inheritdoc />
        </member>
        <member name="T:LLama.Common.MirostatType">
            <summary>
            Type of "mirostat" sampling to use.
            https://github.com/basusourya/mirostat
            </summary>
        </member>
        <member name="F:LLama.Common.MirostatType.Disable">
            <summary>
            Disable Mirostat sampling
            </summary>
        </member>
        <member name="F:LLama.Common.MirostatType.Mirostat">
            <summary>
            Original mirostat algorithm
            </summary>
        </member>
        <member name="F:LLama.Common.MirostatType.Mirostat2">
            <summary>
            Mirostat 2.0 algorithm
            </summary>
        </member>
        <member name="T:LLama.Common.ModelParams">
            <summary>
            The parameters for initializing a LLama model.
            </summary>
        </member>
        <member name="P:LLama.Common.ModelParams.ContextSize">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.MainGpu">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.SplitMode">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.GpuLayerCount">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.Seed">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.UseMemorymap">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.UseMemoryLock">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.ModelPath">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.LoraAdapters">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.LoraBase">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.Threads">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.BatchThreads">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.BatchSize">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.EmbeddingMode">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.TensorSplits">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.MetadataOverrides">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.RopeFrequencyBase">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.RopeFrequencyScale">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.YarnExtrapolationFactor">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.YarnAttentionFactor">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.YarnBetaFast">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.YarnBetaSlow">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.YarnOriginalContext">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.YarnScalingType">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.TypeK">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.TypeV">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.NoKqvOffload">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.VocabOnly">
            <inheritdoc />
        </member>
        <member name="P:LLama.Common.ModelParams.EncodingName">
            <summary>
            `Encoding` cannot be directly JSON serialized, instead store the name as a string which can
            </summary>
        </member>
        <member name="P:LLama.Common.ModelParams.Encoding">
            <inheritdoc />
        </member>
        <member name="M:LLama.Common.ModelParams.#ctor(System.String)">
            <summary>
            
            </summary>
            <param name="modelPath">The model path.</param>
        </member>
        <member name="T:LLama.Exceptions.GrammarFormatException">
            <summary>
            Base class for all grammar exceptions
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarUnexpectedHexCharsCount">
            <summary>
            An incorrect number of characters were encountered while parsing a hex literal
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarExpectedName">
            <summary>
            Failed to parse a "name" element when one was expected
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarUnknownEscapeCharacter">
            <summary>
            An unexpected character was encountered after an escape sequence
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarUnexpectedEndOfInput">
            <summary>
            End-of-file was encountered while parsing
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarExpectedNext">
            <summary>
            A specified string was expected when parsing
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarExpectedPrevious">
            <summary>
            A specified character was expected to preceded another when parsing
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarUnexpectedCharAltElement">
            <summary>
            A CHAR_ALT was created without a preceding CHAR element
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarUnexpectedCharRngElement">
            <summary>
            A CHAR_RNG was created without a preceding CHAR element
            </summary>
        </member>
        <member name="T:LLama.Exceptions.GrammarUnexpectedEndElement">
            <summary>
            An END was encountered before the last element
            </summary>
        </member>
        <member name="T:LLama.Exceptions.RuntimeError">
            <summary>
            Base class for LLamaSharp runtime errors (i.e. errors produced by llama.cpp, converted into exceptions)
            </summary>
        </member>
        <member name="M:LLama.Exceptions.RuntimeError.#ctor(System.String)">
            <summary>
            Create a new RuntimeError
            </summary>
            <param name="message"></param>
        </member>
        <member name="T:LLama.Exceptions.LoadWeightsFailedException">
            <summary>
            Loading model weights failed
            </summary>
        </member>
        <member name="P:LLama.Exceptions.LoadWeightsFailedException.ModelPath">
            <summary>
            The model path which failed to load
            </summary>
        </member>
        <member name="M:LLama.Exceptions.LoadWeightsFailedException.#ctor(System.String)">
            <inheritdoc />
        </member>
        <member name="T:LLama.Exceptions.LLamaDecodeError">
            <summary>
            `llama_decode` return a non-zero status code
            </summary>
        </member>
        <member name="P:LLama.Exceptions.LLamaDecodeError.ReturnCode">
            <summary>
            The return status code
            </summary>
        </member>
        <member name="M:LLama.Exceptions.LLamaDecodeError.#ctor(LLama.Native.DecodeResult)">
            <inheritdoc />
        </member>
        <member name="T:LLama.Extensions.IContextParamsExtensions">
            <summary>
            Extention methods to the IContextParams interface
            </summary>
        </member>
        <member name="M:LLama.Extensions.IContextParamsExtensions.ToLlamaContextParams(LLama.Abstractions.IContextParams,LLama.Native.LLamaContextParams@)">
            <summary>
            Convert the given `IModelParams` into a `LLamaContextParams`
            </summary>
            <param name="params"></param>
            <param name="result"></param>
            <returns></returns>
            <exception cref="T:System.IO.FileNotFoundException"></exception>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="T:LLama.Extensions.IModelParamsExtensions">
            <summary>
            Extention methods to the IModelParams interface
            </summary>
        </member>
        <member name="M:LLama.Extensions.IModelParamsExtensions.ToLlamaModelParams(LLama.Abstractions.IModelParams,LLama.Native.LLamaModelParams@)">
            <summary>
            Convert the given `IModelParams` into a `LLamaModelParams`
            </summary>
            <param name="params"></param>
            <param name="result"></param>
            <returns></returns>
            <exception cref="T:System.IO.FileNotFoundException"></exception>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.Extensions.IReadOnlyListExtensions.IndexOf``1(System.Collections.Generic.IReadOnlyList{``0},``0)">
            <summary>
            Find the index of `item` in `list`
            </summary>
            <typeparam name="T"></typeparam>
            <param name="list">list to search</param>
            <param name="item">item to search for</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Extensions.IReadOnlyListExtensions.TokensEndsWithAnyString``2(``0,``1,LLama.Native.SafeLlamaModelHandle,System.Text.Encoding)">
            <summary>
            Check if the given set of tokens ends with any of the given strings
            </summary>
            <param name="tokens">Tokens to check</param>
            <param name="queries">Strings to search for</param>
            <param name="model">Model to use to convert tokens into bytes</param>
            <param name="encoding">Encoding to use to convert bytes into characters</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Extensions.IReadOnlyListExtensions.TokensEndsWithAnyString``1(``0,System.Collections.Generic.IList{System.String},LLama.Native.SafeLlamaModelHandle,System.Text.Encoding)">
            <summary>
            Check if the given set of tokens ends with any of the given strings
            </summary>
            <param name="tokens">Tokens to check</param>
            <param name="queries">Strings to search for</param>
            <param name="model">Model to use to convert tokens into bytes</param>
            <param name="encoding">Encoding to use to convert bytes into characters</param>
            <returns></returns>
        </member>
        <member name="T:LLama.Extensions.KeyValuePairExtensions">
            <summary>
            Extensions to the KeyValuePair struct
            </summary>
        </member>
        <member name="M:LLama.Extensions.KeyValuePairExtensions.Deconstruct``2(System.Collections.Generic.KeyValuePair{``0,``1},``0@,``1@)">
            <summary>
            Deconstruct a KeyValuePair into it's constituent parts.
            </summary>
            <param name="pair">The KeyValuePair to deconstruct</param>
            <param name="first">First element, the Key</param>
            <param name="second">Second element, the Value</param>
            <typeparam name="TKey">Type of the Key</typeparam>
            <typeparam name="TValue">Type of the Value</typeparam>
        </member>
        <member name="T:LLama.Grammars.GBNFGrammarParser">
            <summary>
            Source:
            https://github.com/ggerganov/llama.cpp/blob/6381d4e110bd0ec02843a60bbeb8b6fc37a9ace9/common/grammar-parser.cpp
            
            The commit hash from URL is the actual commit hash that reflects current C# code.
            </summary>
        </member>
        <member name="M:LLama.Grammars.GBNFGrammarParser.Parse(System.String,System.String)">
            <summary>
            Parse a string of <a href="https://github.com/ggerganov/llama.cpp/tree/master/grammars">GGML BNF</a>
            </summary>
            <param name="input">The string to parse</param>
            <param name="startRule">The name of the root rule of this grammar</param>
            <exception cref="T:LLama.Exceptions.GrammarFormatException">Thrown if input is malformed</exception>
            <returns>A ParseState that can be converted into a grammar for sampling</returns>
        </member>
        <member name="T:LLama.Grammars.Grammar">
            <summary>
            A grammar is a set of <see cref="T:LLama.Grammars.GrammarRule"/>s for deciding which characters are valid next. Can be used to constrain
            output to certain formats - e.g. force the model to output JSON
            </summary>
        </member>
        <member name="P:LLama.Grammars.Grammar.StartRuleIndex">
            <summary>
            Index of the initial rule to start from
            </summary>
        </member>
        <member name="P:LLama.Grammars.Grammar.Rules">
            <summary>
            The rules which make up this grammar
            </summary>
        </member>
        <member name="M:LLama.Grammars.Grammar.#ctor(System.Collections.Generic.IReadOnlyList{LLama.Grammars.GrammarRule},System.UInt64)">
            <summary>
            Create a new grammar from a set of rules
            </summary>
            <param name="rules">The rules which make up this grammar</param>
            <param name="startRuleIndex">Index of the initial rule to start from</param>
            <exception cref="T:System.ArgumentOutOfRangeException"></exception>
        </member>
        <member name="M:LLama.Grammars.Grammar.CreateInstance">
            <summary>
            Create a `SafeLLamaGrammarHandle` instance to use for parsing
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Grammars.Grammar.Parse(System.String,System.String)">
            <summary>
            Parse a string of <a href="https://github.com/ggerganov/llama.cpp/tree/master/grammars">GGML BNF</a> into a Grammar
            </summary>
            <param name="gbnf">The string to parse</param>
            <param name="startRule">Name of the start rule of this grammar</param>
            <exception cref="T:LLama.Exceptions.GrammarFormatException">Thrown if input is malformed</exception>
            <returns>A Grammar which can be converted into a SafeLLamaGrammarHandle for sampling</returns>
        </member>
        <member name="M:LLama.Grammars.Grammar.ToString">
            <inheritdoc />
        </member>
        <member name="T:LLama.Grammars.GrammarRule">
            <summary>
            A single rule in a <see cref="T:LLama.Grammars.Grammar"/>
            </summary>
        </member>
        <member name="P:LLama.Grammars.GrammarRule.Name">
            <summary>
            Name of this rule
            </summary>
        </member>
        <member name="P:LLama.Grammars.GrammarRule.Elements">
            <summary>
            The elements of this grammar rule
            </summary>
        </member>
        <member name="M:LLama.Grammars.GrammarRule.#ctor(System.String,System.Collections.Generic.IReadOnlyList{LLama.Native.LLamaGrammarElement})">
            <summary>
            Create a new GrammarRule containing the given elements
            </summary>
            <param name="name"></param>
            <param name="elements"></param>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="T:LLama.LLamaContext">
            <summary>
            A llama_context, which holds all the context required to interact with a model
            </summary>
        </member>
        <member name="P:LLama.LLamaContext.VocabCount">
            <summary>
            Total number of tokens in vocabulary of this model
            </summary>
        </member>
        <member name="P:LLama.LLamaContext.ContextSize">
            <summary>
            Total number of tokens in the context
            </summary>
        </member>
        <member name="P:LLama.LLamaContext.EmbeddingSize">
            <summary>
            Dimension of embedding vectors
            </summary>
        </member>
        <member name="P:LLama.LLamaContext.Params">
            <summary>
            The context params set for this context
            </summary>
        </member>
        <member name="P:LLama.LLamaContext.NativeHandle">
            <summary>
            The native handle, which is used to be passed to the native APIs
            </summary>
            <remarks>Be careful how you use this!</remarks>
        </member>
        <member name="P:LLama.LLamaContext.Encoding">
            <summary>
            The encoding set for this model to deal with text input.
            </summary>
        </member>
        <member name="M:LLama.LLamaContext.#ctor(LLama.LLamaWeights,LLama.Abstractions.IContextParams,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            Create a new LLamaContext for the given LLamaWeights
            </summary>
            <param name="model"></param>
            <param name="params"></param>
            <param name="logger"></param>
            <exception cref="T:System.ObjectDisposedException"></exception>
        </member>
        <member name="M:LLama.LLamaContext.SetSeed(System.UInt32)">
            <summary>
            Set the seed for the RNG
            </summary>
            <param name="seed"></param>
        </member>
        <member name="M:LLama.LLamaContext.Tokenize(System.String,System.Boolean,System.Boolean)">
            <summary>
            Tokenize a string.
            </summary>
            <param name="text"></param>
            <param name="addBos">Whether to add a bos to the text.</param>
            <param name="special">Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</param>
            <returns></returns>
        </member>
        <member name="M:LLama.LLamaContext.DeTokenize(System.Collections.Generic.IReadOnlyList{LLama.Native.LLamaToken})">
            <summary>
            Detokenize the tokens to text.
            </summary>
            <param name="tokens"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.LLamaContext.SaveState(System.String)">
            <summary>
            Save the state to specified path.
            </summary>
            <param name="filename"></param>
        </member>
        <member name="M:LLama.LLamaContext.GetState">
            <summary>
            Get the state data as an opaque handle
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.LLamaContext.LoadState(System.String)">
            <summary>
            Load the state from specified path.
            </summary>
            <param name="filename"></param>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.LLamaContext.LoadState(LLama.LLamaContext.State)">
            <summary>
            Load the state from memory.
            </summary>
            <param name="state"></param>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.LLamaContext.Sample(LLama.Sampling.ISamplingPipeline,System.ReadOnlySpan{LLama.Native.LLamaToken})">
            <summary>
            Sample a single token from this context, using the given sampling pipeline
            </summary>
            <param name="pipeline">The pipeline to use to process the logits and to select a token</param>
            <param name="lastTokens">The tokens recently returned from the model</param>
            <returns>The selected token</returns>
        </member>
        <member name="M:LLama.LLamaContext.Sample(LLama.Native.LLamaTokenDataArray,System.Nullable{System.Single}@,System.Single,LLama.Common.MirostatType,System.Single,System.Single,System.Int32,System.Single,System.Single,System.Single,LLama.Native.SafeLLamaGrammarHandle,System.Single)">
            <summary>
            Perform the sampling. Please don't use it unless you fully know what it does.
            </summary>
            <param name="candidates"></param>
            <param name="mirostat_mu"></param>
            <param name="temperature"></param>
            <param name="mirostat"></param>
            <param name="mirostatTau"></param>
            <param name="mirostatEta"></param>
            <param name="topK"></param>
            <param name="topP"></param>
            <param name="tfsZ"></param>
            <param name="typicalP"></param>
            <param name="grammar"></param>
            <param name="minP"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.LLamaContext.ApplyPenalty(System.Int32,System.Collections.Generic.IEnumerable{LLama.Native.LLamaToken},System.Collections.Generic.Dictionary{LLama.Native.LLamaToken,System.Single},System.Int32,System.Single,System.Single,System.Single,System.Boolean)">
            <summary>
            Apply the penalty for the tokens. Please don't use it unless you fully know what it does.
            </summary>
            <param name="logits_i"></param>
            <param name="lastTokens"></param>
            <param name="logitBias"></param>
            <param name="repeatLastTokensCount"></param>
            <param name="repeatPenalty"></param>
            <param name="alphaFrequency"></param>
            <param name="alphaPresence"></param>
            <param name="penalizeNL"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.LLamaContext.Decode(LLama.Native.LLamaBatch)">
            <summary>
            </summary>
            <param name="batch"></param>
        </member>
        <member name="M:LLama.LLamaContext.DecodeAsync(LLama.Native.LLamaBatch,System.Threading.CancellationToken)">
            <summary>
            </summary>
            <param name="batch"></param>
            <param name="cancellationToken"></param>
        </member>
        <member name="M:LLama.LLamaContext.Eval(System.Collections.Generic.List{LLama.Native.LLamaToken},System.Int32)">
            <summary>
            
            </summary>
            <param name="tokens"></param>
            <param name="pastTokensCount"></param>
            <returns>The updated `pastTokensCount`.</returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.LLamaContext.Eval(System.ReadOnlySpan{LLama.Native.LLamaToken},System.Int32)">
            <summary>
            
            </summary>
            <param name="tokens"></param>
            <param name="pastTokensCount"></param>
            <returns>The updated `pastTokensCount`.</returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.LLamaContext.Dispose">
            <inheritdoc />
        </member>
        <member name="T:LLama.LLamaContext.State">
            <summary>
            The state of this model, which can be reloaded later
            </summary>
        </member>
        <member name="M:LLama.LLamaContext.State.ReleaseHandle">
            <inheritdoc />
        </member>
        <member name="T:LLama.LLamaEmbedder">
            <summary>
            The embedder for LLama, which supports getting embeddings from text.
            </summary>
        </member>
        <member name="P:LLama.LLamaEmbedder.EmbeddingSize">
            <summary>
            Dimension of embedding vectors
            </summary>
        </member>
        <member name="P:LLama.LLamaEmbedder.Context">
            <summary>
            LLama Context
            </summary>
        </member>
        <member name="M:LLama.LLamaEmbedder.#ctor(LLama.LLamaWeights,LLama.Abstractions.IContextParams,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            Create a new embedder, using the given LLamaWeights
            </summary>
            <param name="weights"></param>
            <param name="params"></param>
            <param name="logger"></param>
        </member>
        <member name="M:LLama.LLamaEmbedder.GetEmbeddings(System.String,System.Threading.CancellationToken)">
            <summary>
            Get the embeddings of the text.
            </summary>
            <param name="text"></param>
            <param name="cancellationToken"></param>
            <returns></returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.LLamaEmbedder.GetEmbeddings(System.String,System.Boolean,System.Threading.CancellationToken)">
            <summary>
            Get the embeddings of the text.
            </summary>
            <param name="text"></param>
            <param name="addBos">Add bos to the text.</param>
            <param name="cancellationToken"></param>
            <returns></returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.LLamaEmbedder.Dispose">
            <inheritdoc />
        </member>
        <member name="T:LLama.StatefulExecutorBase">
            <summary>
            The base class for stateful LLama executors.
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._logger">
            <summary>
            The logger used by this executor.
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._pastTokensCount">
            <summary>
            The tokens that were already processed by the model.
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._consumedTokensCount">
            <summary>
            The tokens that were consumed by the model during the current inference.
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._n_session_consumed">
            <summary>
            
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._n_matching_session_tokens">
            <summary>
            
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._pathSession">
            <summary>
            The path of the session file.
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._embeds">
            <summary>
            A container of the tokens to be processed and after processed.
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._embed_inps">
            <summary>
            A container for the tokens of input.
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._session_tokens">
            <summary>
            
            </summary>
        </member>
        <member name="F:LLama.StatefulExecutorBase._last_n_tokens">
            <summary>
            The last tokens generated by the model.
            </summary>
        </member>
        <member name="P:LLama.StatefulExecutorBase.Context">
            <summary>
            The context used by the executor.
            </summary>
        </member>
        <member name="P:LLama.StatefulExecutorBase.MirostatMu">
            <summary>
            Current "mu" value for mirostat sampling
            </summary>
        </member>
        <member name="M:LLama.StatefulExecutorBase.#ctor(LLama.LLamaContext,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            
            </summary>
            <param name="context"></param>
            <param name="logger"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.WithSessionFile(System.String)">
            <summary>
            This API is currently not verified.
            </summary>
            <param name="filename"></param>
            <returns></returns>
            <exception cref="T:System.ArgumentNullException"></exception>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.StatefulExecutorBase.SaveSessionFile(System.String)">
            <summary>
            This API has not been verified currently.
            </summary>
            <param name="filename"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.HandleRunOutOfContext(System.Int32)">
            <summary>
            After running out of the context, take some tokens from the original prompt and recompute the logits in batches.
            </summary>
            <param name="tokensToKeep"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.TryReuseMathingPrefix">
            <summary>
            Try to reuse the matching prefix from the session file.
            </summary>
        </member>
        <member name="M:LLama.StatefulExecutorBase.GetLoopCondition(LLama.StatefulExecutorBase.InferStateArgs)">
            <summary>
            Decide whether to continue the loop.
            </summary>
            <param name="args"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.StatefulExecutorBase.PreprocessInputs(System.String,LLama.StatefulExecutorBase.InferStateArgs)">
            <summary>
            Preprocess the inputs before the inference.
            </summary>
            <param name="text"></param>
            <param name="args"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.PostProcess(LLama.Abstractions.IInferenceParams,LLama.StatefulExecutorBase.InferStateArgs)">
            <summary>
            Do some post processing after the inference.
            </summary>
            <param name="inferenceParams"></param>
            <param name="args"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.StatefulExecutorBase.InferInternal(LLama.Abstractions.IInferenceParams,LLama.StatefulExecutorBase.InferStateArgs)">
            <summary>
            The core inference logic.
            </summary>
            <param name="inferenceParams"></param>
            <param name="args"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.SaveState(System.String)">
            <summary>
            Save the current state to a file.
            </summary>
            <param name="filename"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.GetStateData">
            <summary>
            Get the current state data.
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.StatefulExecutorBase.LoadState(LLama.StatefulExecutorBase.ExecutorBaseState)">
            <summary>
            Load the state from data.
            </summary>
            <param name="data"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.LoadState(System.String)">
            <summary>
            Load the state from a file.
            </summary>
            <param name="filename"></param>
        </member>
        <member name="M:LLama.StatefulExecutorBase.InferAsync(System.String,LLama.Abstractions.IInferenceParams,System.Threading.CancellationToken)">
            <summary>
            Execute the inference.
            </summary>
            <param name="text"></param>
            <param name="inferenceParams"></param>
            <param name="cancellationToken"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.StatefulExecutorBase.InferStateArgs">
            <summary>
            State arguments that are used in single inference
            </summary>
        </member>
        <member name="P:LLama.StatefulExecutorBase.InferStateArgs.Antiprompts">
            <summary>
            
            </summary>
        </member>
        <member name="P:LLama.StatefulExecutorBase.InferStateArgs.RemainedTokens">
            <summary>
            Tokens count remained to be used. (n_remain)
            </summary>
        </member>
        <member name="P:LLama.StatefulExecutorBase.InferStateArgs.ReturnValue">
            <summary>
            
            </summary>
        </member>
        <member name="P:LLama.StatefulExecutorBase.InferStateArgs.WaitForInput">
            <summary>
            
            </summary>
        </member>
        <member name="P:LLama.StatefulExecutorBase.InferStateArgs.NeedToSaveSession">
            <summary>
            
            </summary>
        </member>
        <member name="T:LLama.InstructExecutor">
            <summary>
            The LLama executor for instruct mode.
            </summary>
        </member>
        <member name="M:LLama.InstructExecutor.#ctor(LLama.LLamaContext,System.String,System.String,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            
            </summary>
            <param name="context"></param>
            <param name="instructionPrefix"></param>
            <param name="instructionSuffix"></param>
            <param name="logger"></param>
        </member>
        <member name="M:LLama.InstructExecutor.GetStateData">
            <inheritdoc />
        </member>
        <member name="M:LLama.InstructExecutor.LoadState(LLama.StatefulExecutorBase.ExecutorBaseState)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InstructExecutor.SaveState(System.String)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InstructExecutor.LoadState(System.String)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InstructExecutor.GetLoopCondition(LLama.StatefulExecutorBase.InferStateArgs)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InstructExecutor.PreprocessInputs(System.String,LLama.StatefulExecutorBase.InferStateArgs)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InstructExecutor.PostProcess(LLama.Abstractions.IInferenceParams,LLama.StatefulExecutorBase.InferStateArgs)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InstructExecutor.InferInternal(LLama.Abstractions.IInferenceParams,LLama.StatefulExecutorBase.InferStateArgs)">
            <inheritdoc />
        </member>
        <member name="T:LLama.InstructExecutor.InstructExecutorState">
            <summary>
            The desciptor of the state of the instruct executor.
            </summary>
        </member>
        <member name="P:LLama.InstructExecutor.InstructExecutorState.IsPromptRun">
            <summary>
            Whether the executor is running for the first time (running the prompt).
            </summary>
        </member>
        <member name="P:LLama.InstructExecutor.InstructExecutorState.InputPrefixTokens">
            <summary>
            Instruction prefix tokens.
            </summary>
        </member>
        <member name="P:LLama.InstructExecutor.InstructExecutorState.InputSuffixTokens">
            <summary>
            Instruction suffix tokens.
            </summary>
        </member>
        <member name="T:LLama.InteractiveExecutor">
            <summary>
            The LLama executor for interactive mode.
            </summary>
        </member>
        <member name="M:LLama.InteractiveExecutor.#ctor(LLama.LLamaContext,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            
            </summary>
            <param name="context"></param>
            <param name="logger"></param>
        </member>
        <member name="M:LLama.InteractiveExecutor.GetStateData">
            <inheritdoc />
        </member>
        <member name="M:LLama.InteractiveExecutor.LoadState(LLama.StatefulExecutorBase.ExecutorBaseState)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InteractiveExecutor.SaveState(System.String)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InteractiveExecutor.LoadState(System.String)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InteractiveExecutor.GetLoopCondition(LLama.StatefulExecutorBase.InferStateArgs)">
            <summary>
            Define whether to continue the loop to generate responses.
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.InteractiveExecutor.PreprocessInputs(System.String,LLama.StatefulExecutorBase.InferStateArgs)">
            <inheritdoc />
        </member>
        <member name="M:LLama.InteractiveExecutor.PostProcess(LLama.Abstractions.IInferenceParams,LLama.StatefulExecutorBase.InferStateArgs)">
            <summary>
            Return whether to break the generation.
            </summary>
            <param name="inferenceParams"></param>
            <param name="args"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.InteractiveExecutor.InferInternal(LLama.Abstractions.IInferenceParams,LLama.StatefulExecutorBase.InferStateArgs)">
            <inheritdoc />
        </member>
        <member name="T:LLama.InteractiveExecutor.InteractiveExecutorState">
            <summary>
            The descriptor of the state of the interactive executor.
            </summary>
        </member>
        <member name="P:LLama.InteractiveExecutor.InteractiveExecutorState.IsPromptRun">
            <summary>
            Whether the executor is running for the first time (running the prompt).
            </summary>
        </member>
        <member name="T:LLama.LLamaQuantizer">
            <summary>
            The quantizer to quantize the model.
            </summary>
        </member>
        <member name="M:LLama.LLamaQuantizer.Quantize(System.String,System.String,LLama.Native.LLamaFtype,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            Quantize the model.
            </summary>
            <param name="srcFileName">The model file to be quantized.</param>
            <param name="dstFilename">The path to save the quantized model.</param>
            <param name="ftype">The type of quantization.</param>
            <param name="nthread">Thread to be used during the quantization. By default it's the physical core number.</param>
            <param name="allowRequantize"></param>
            <param name="quantizeOutputTensor"></param>
            <returns>Whether the quantization is successful.</returns>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.LLamaQuantizer.Quantize(System.String,System.String,System.String,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            Quantize the model.
            </summary>
            <param name="srcFileName">The model file to be quantized.</param>
            <param name="dstFilename">The path to save the quantized model.</param>
            <param name="ftype">The type of quantization.</param>
            <param name="nthread">Thread to be used during the quantization. By default it's the physical core number.</param>
            <param name="allowRequantize"></param>
            <param name="quantizeOutputTensor"></param>
            <returns>Whether the quantization is successful.</returns>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:LLama.LLamaQuantizer.StringToFtype(System.String)">
             <summary>
             Parse a string into a LLamaFtype. This is a "relaxed" parsing, which allows any string which is contained within
             the enum name to be used.
            
             For example "Q5_K_M" will convert to "LLAMA_FTYPE_MOSTLY_Q5_K_M"
             </summary>
             <param name="str"></param>
             <returns></returns>
             <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="T:LLama.StatelessExecutor">
            <summary>
            This executor infer the input as one-time job. Previous inputs won't impact on the 
            response to current input.
            </summary>
        </member>
        <member name="P:LLama.StatelessExecutor.Context">
            <summary>
            The context used by the executor when running the inference.
            </summary>
        </member>
        <member name="M:LLama.StatelessExecutor.#ctor(LLama.LLamaWeights,LLama.Abstractions.IContextParams,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            Create a new stateless executor which will use the given model
            </summary>
            <param name="weights"></param>
            <param name="params"></param>
            <param name="logger"></param>
        </member>
        <member name="M:LLama.StatelessExecutor.InferAsync(System.String,LLama.Abstractions.IInferenceParams,System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="T:LLama.LLamaTransforms">
            <summary>
            A class that contains all the transforms provided internally by LLama.
            </summary>
        </member>
        <member name="T:LLama.LLamaTransforms.DefaultHistoryTransform">
            <summary>
            The default history transform.
            Uses plain text with the following format:
            [Author]: [Message]
            </summary>
        </member>
        <member name="M:LLama.LLamaTransforms.DefaultHistoryTransform.#ctor(System.String,System.String,System.String,System.String,System.Boolean)">
            <summary>
            
            </summary>
            <param name="userName"></param>
            <param name="assistantName"></param>
            <param name="systemName"></param>
            <param name="unknownName"></param>
            <param name="isInstructMode"></param>
        </member>
        <member name="M:LLama.LLamaTransforms.DefaultHistoryTransform.HistoryToText(LLama.Common.ChatHistory)">
            <inheritdoc />
        </member>
        <member name="M:LLama.LLamaTransforms.DefaultHistoryTransform.TextToHistory(LLama.Common.AuthorRole,System.String)">
            <inheritdoc />
        </member>
        <member name="M:LLama.LLamaTransforms.DefaultHistoryTransform.TrimNamesFromText(System.String,LLama.Common.AuthorRole)">
            <summary>
            Drop the name at the beginning and the end of the text.
            </summary>
            <param name="text"></param>
            <param name="role"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.LLamaTransforms.NaiveTextInputTransform">
            <summary>
            A text input transform that only trims the text.
            </summary>
        </member>
        <member name="M:LLama.LLamaTransforms.NaiveTextInputTransform.Transform(System.String)">
            <inheritdoc />
        </member>
        <member name="T:LLama.LLamaTransforms.EmptyTextOutputStreamTransform">
            <summary>
            A no-op text input transform.
            </summary>
        </member>
        <member name="M:LLama.LLamaTransforms.EmptyTextOutputStreamTransform.TransformAsync(System.Collections.Generic.IAsyncEnumerable{System.String})">
            <inheritdoc />
        </member>
        <member name="T:LLama.LLamaTransforms.KeywordTextOutputStreamTransform">
            <summary>
            A text output transform that removes the keywords from the response.
            </summary>
        </member>
        <member name="M:LLama.LLamaTransforms.KeywordTextOutputStreamTransform.#ctor(System.Collections.Generic.IEnumerable{System.String},System.Int32,System.Boolean)">
            <summary>
            
            </summary>
            <param name="keywords">Keywords that you want to remove from the response.</param>
            <param name="redundancyLength">The extra length when searching for the keyword. For example, if your only keyword is "highlight", 
            maybe the token you get is "\r\nhighligt". In this condition, if redundancyLength=0, the token cannot be successfully matched because the length of "\r\nhighligt" (10)
            has already exceeded the maximum length of the keywords (8). On the contrary, setting redundancyLengyh &gt;= 2 leads to successful match.
            The larger the redundancyLength is, the lower the processing speed. But as an experience, it won't introduce too much performance impact when redundancyLength &lt;= 5 </param>
            <param name="removeAllMatchedTokens">If set to true, when getting a matched keyword, all the related tokens will be removed. Otherwise only the part of keyword will be removed.</param>
        </member>
        <member name="M:LLama.LLamaTransforms.KeywordTextOutputStreamTransform.TransformAsync(System.Collections.Generic.IAsyncEnumerable{System.String})">
            <inheritdoc />
        </member>
        <member name="T:LLama.LLamaWeights">
            <summary>
            A set of model weights, loaded into memory.
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.NativeHandle">
            <summary>
            The native handle, which is used in the native APIs
            </summary>
            <remarks>Be careful how you use this!</remarks>
        </member>
        <member name="P:LLama.LLamaWeights.VocabCount">
            <summary>
            Total number of tokens in vocabulary of this model
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.ContextSize">
            <summary>
            Total number of tokens in the context
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.SizeInBytes">
            <summary>
            Get the size of this model in bytes
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.ParameterCount">
            <summary>
            Get the number of parameters in this model
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.NewlineToken">
            <summary>
            Get the newline token for this model
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.EndOfSentenceToken">
            <summary>
            Get the "end of sentence" token for this model
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.BeginningOfSentenceToken">
            <summary>
            Get the "beginning of sentence" token for this model
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.EmbeddingSize">
            <summary>
            Dimension of embedding vectors
            </summary>
        </member>
        <member name="P:LLama.LLamaWeights.Metadata">
            <summary>
            All metadata keys in this model
            </summary>
        </member>
        <member name="M:LLama.LLamaWeights.LoadFromFile(LLama.Abstractions.IModelParams)">
            <summary>
            Load weights into memory
            </summary>
            <param name="params"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.LLamaWeights.Dispose">
            <inheritdoc />
        </member>
        <member name="M:LLama.LLamaWeights.CreateContext(LLama.Abstractions.IContextParams,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            Create a llama_context using this model
            </summary>
            <param name="params"></param>
            <param name="logger"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.LLamaWeights.Tokenize(System.String,System.Boolean,System.Boolean,System.Text.Encoding)">
            <summary>
            Convert a string of text into tokens
            </summary>
            <param name="text"></param>
            <param name="add_bos"></param>
            <param name="encoding"></param>
            <param name="special">Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</param>
            <returns></returns>
        </member>
        <member name="T:LLama.Native.DecodeResult">
            <summary>
            Return codes from llama_decode
            </summary>
        </member>
        <member name="F:LLama.Native.DecodeResult.Error">
            <summary>
            An unspecified error
            </summary>
        </member>
        <member name="F:LLama.Native.DecodeResult.Ok">
            <summary>
            Ok.
            </summary>
        </member>
        <member name="F:LLama.Native.DecodeResult.NoKvSlot">
            <summary>
            Could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
            </summary>
        </member>
        <member name="T:LLama.Native.GGMLType">
            <summary>
            Possible GGML quantisation types
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_F32">
            <summary>
            Full 32 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_F16">
            <summary>
            16 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q4_0">
            <summary>
            4 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q4_1">
            <summary>
            4 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q5_0">
            <summary>
            5 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q5_1">
            <summary>
            5 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q8_0">
            <summary>
            8 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q8_1">
            <summary>
            8 bit float
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q2_K">
            <summary>
            "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight.
            Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q3_K">
            <summary>
            "type-0" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights.
            Scales are quantized with 6 bits. This end up using 3.4375 bpw.
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q4_K">
            <summary>
            "type-1" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights.
            Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q5_K">
            <summary>
            "type-1" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q6_K">
            <summary>
            "type-0" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights.
            Scales are quantized with 8 bits. This ends up using 6.5625 bpw
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_Q8_K">
            <summary>
            "type-0" 8-bit quantization. Only used for quantizing intermediate results.
            The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_I8">
            <summary>
            Integer, 8 bit
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_I16">
            <summary>
            Integer, 16 bit
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_I32">
            <summary>
            Integer, 32 bit
            </summary>
        </member>
        <member name="F:LLama.Native.GGMLType.GGML_TYPE_COUNT">
            <summary>
            The value of this entry is the count of the number of possible quant types.
            </summary>
        </member>
        <member name="T:LLama.Native.GPUSplitMode">
            <summary>
            
            </summary>
            <remarks>llama_split_mode</remarks>
        </member>
        <member name="F:LLama.Native.GPUSplitMode.None">
            <summary>
            Single GPU
            </summary>
        </member>
        <member name="F:LLama.Native.GPUSplitMode.Layer">
            <summary>
            Split layers and KV across GPUs
            </summary>
        </member>
        <member name="F:LLama.Native.GPUSplitMode.Row">
            <summary>
            split rows across GPUs
            </summary>
        </member>
        <member name="T:LLama.Native.GroupDisposable">
            <summary>
            Disposes all contained disposables when this class is disposed
            </summary>
        </member>
        <member name="M:LLama.Native.GroupDisposable.Finalize">
            <inheritdoc />
        </member>
        <member name="M:LLama.Native.GroupDisposable.Dispose">
            <inheritdoc />
        </member>
        <member name="T:LLama.Native.LLamaBatch">
            <summary>
            A batch allows submitting multiple tokens to multiple sequences simultaneously
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaBatch._index">
            <summary>
            Keep track of the index of existing token/position combos in the batch
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaBatch.TokenCount">
            <summary>
            The number of tokens in this batch
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaBatch.TokenCapacity">
            <summary>
            Maximum number of tokens that can be added to this batch (automatically grows if exceeded)
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaBatch.SequenceCapacity">
            <summary>
            Maximum number of sequences a token can be assigned to (automatically grows if exceeded)
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaBatch.#ctor">
            <summary>
            Create a new batch for submitting inputs to llama.cpp
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaBatch.Add(LLama.Native.LLamaToken,LLama.Native.LLamaPos,System.ReadOnlySpan{LLama.Native.LLamaSeqId},System.Boolean)">
            <summary>
            Add a single token to the batch at the same position in several sequences
            </summary>
            <remarks>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</remarks>
            <param name="token">The token to add</param>
            <param name="pos">The position to add it att</param>
            <param name="sequences">The set of sequences to add this token to</param>
            <param name="logits"></param>
            <returns>The index that the token was added at. Use this for GetLogitsIth</returns>
        </member>
        <member name="M:LLama.Native.LLamaBatch.Add(LLama.Native.LLamaToken,LLama.Native.LLamaPos,System.Collections.Generic.List{LLama.Native.LLamaSeqId},System.Boolean)">
            <summary>
            Add a single token to the batch at the same position in several sequences
            </summary>
            <remarks>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</remarks>
            <param name="token">The token to add</param>
            <param name="pos">The position to add it att</param>
            <param name="sequences">The set of sequences to add this token to</param>
            <param name="logits"></param>
            <returns>The index that the token was added at. Use this for GetLogitsIth</returns>
        </member>
        <member name="M:LLama.Native.LLamaBatch.Add(LLama.Native.LLamaToken,LLama.Native.LLamaPos,LLama.Native.LLamaSeqId,System.Boolean)">
            <summary>
            Add a single token to the batch at a certain position for a single sequences
            </summary>
            <remarks>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</remarks>
            <param name="token">The token to add</param>
            <param name="pos">The position to add it att</param>
            <param name="sequence">The sequence to add this token to</param>
            <param name="logits"></param>
            <returns>The index that the token was added at. Use this for GetLogitsIth</returns>
        </member>
        <member name="M:LLama.Native.LLamaBatch.AddRange(System.ReadOnlySpan{LLama.Native.LLamaToken},LLama.Native.LLamaPos,LLama.Native.LLamaSeqId,System.Boolean)">
            <summary>
            Add a range of tokens to a single sequence, start at the given position.
            </summary>
            <param name="tokens">The tokens to add</param>
            <param name="start">The starting position to add tokens at</param>
            <param name="sequence">The sequence to add this token to</param>
            <param name="logitsLast">Whether the final token should generate logits</param>
            <returns>The index that the final token was added at. Use this for GetLogitsIth</returns>
        </member>
        <member name="M:LLama.Native.LLamaBatch.Clear">
            <summary>
            Set TokenCount to zero for this batch
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaBeamsState">
            <summary>
            Passed to beam_search_callback function.
            Whenever 0 &lt; common_prefix_length, this number of tokens should be copied from any of the beams
            (e.g. beams[0]) as they will be removed (shifted) from all beams in all subsequent callbacks.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaBeamsState.beam_views">
            <summary>
            The state of each individual beam
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaBeamsState.n_beams">
            <summary>
            Number of elements in beam_views
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaBeamsState.CommonPrefixLength">
            <summary>
            Current max length of prefix tokens shared by all beams.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaBeamsState.LastCall">
            <summary>
            True iff this is the last callback invocation.
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaBeamsState.Beams">
            <summary>
            The current state of each beam
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaBeamView">
            <summary>
            Information about a single beam in a beam search
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaBeamView.CumulativeProbability">
            <summary>
            Cumulative beam probability (renormalized relative to all beams)
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaBeamView.EndOfBeam">
            <summary>
            Callback should set this to true when a beam is at end-of-beam.
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaBeamView.Tokens">
            <summary>
            Tokens in this beam
            </summary>
        </member>
        <member name="T:LLama.Native.LlamaProgressCallback">
            <summary>
            Called by llama.cpp with a progress value between 0 and 1
            </summary>
            <param name="progress"></param>
            <param name="ctx"></param>
            <remarks>llama_progress_callback</remarks>
        </member>
        <member name="T:LLama.Native.LLamaContextParams">
            <summary>
            A C# representation of the llama.cpp `llama_context_params` struct
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.seed">
            <summary>
            RNG seed, -1 for random
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.n_ctx">
            <summary>
            text context, 0 = from model
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.n_batch">
            <summary>
            prompt processing batch size
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.n_threads">
            <summary>
            number of threads to use for generation
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.n_threads_batch">
            <summary>
            number of threads to use for batch processing
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.rope_scaling_type">
            <summary>
            RoPE scaling type, from `enum llama_rope_scaling_type` 
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.rope_freq_base">
            <summary>
            RoPE base frequency, 0 = from model
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.rope_freq_scale">
            <summary>
            RoPE frequency scaling factor, 0 = from model
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.yarn_ext_factor">
            <summary>
            YaRN extrapolation mix factor, negative = from model
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.yarn_attn_factor">
            <summary>
            YaRN magnitude scaling factor
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.yarn_beta_fast">
            <summary>
            YaRN low correction dim
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.yarn_beta_slow">
            <summary>
            YaRN high correction dim
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.yarn_orig_ctx">
            <summary>
            YaRN original context size
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.cb_eval">
            <summary>
            ggml_backend_sched_eval_callback
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.cb_eval_user_data">
            <summary>
            User data passed into cb_eval
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.type_k">
            <summary>
            data type for K cache
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams.type_v">
            <summary>
            data type for V cache
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams._mul_mat_q">
            <summary>
            Deprecated!
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaContextParams._logits_all">
            <summary>
            Deprecated!
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaContextParams.embedding">
            <summary>
            embedding mode only
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaContextParams.offload_kqv">
            <summary>
            whether to offload the KQV ops (including the KV cache) to GPU
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaFtype">
            <summary>
            Supported model file types
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_ALL_F32">
            <summary>
            All f32
            </summary>
            <remarks>Benchmark@7B: 26GB</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_F16">
            <summary>
            Mostly f16
            </summary>
            <remarks>Benchmark@7B: 13GB</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q8_0">
            <summary>
            Mostly 8 bit
            </summary>
            <remarks>Benchmark@7B: 6.7GB, +0.0004ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q4_0">
            <summary>
            Mostly 4 bit
            </summary>
            <remarks>Benchmark@7B: 3.50GB, +0.2499 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q4_1">
            <summary>
            Mostly 4 bit
            </summary>
            <remarks>Benchmark@7B: 3.90GB, +0.1846 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16">
            <summary>
            Mostly 4 bit, tok_embeddings.weight and output.weight are f16
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q5_0">
            <summary>
            Mostly 5 bit
            </summary>
            <remarks>Benchmark@7B: 4.30GB @ 7B tokens, +0.0796 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q5_1">
            <summary>
            Mostly 5 bit
            </summary>
            <remarks>Benchmark@7B: 4.70GB, +0.0415 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q2_K">
            <summary>
            K-Quant 2 bit
            </summary>
            <remarks>Benchmark@7B: 2.67GB @ 7N parameters, +0.8698 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q3_K_S">
            <summary>
            K-Quant 3 bit (Small)
            </summary>
            <remarks>Benchmark@7B: 2.75GB, +0.5505 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q3_K_M">
            <summary>
            K-Quant 3 bit (Medium)
            </summary>
            <remarks>Benchmark@7B: 3.06GB, +0.2437 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q3_K_L">
            <summary>
            K-Quant 3 bit (Large)
            </summary>
            <remarks>Benchmark@7B: 3.35GB, +0.1803 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q4_K_S">
            <summary>
            K-Quant 4 bit (Small)
            </summary>
            <remarks>Benchmark@7B: 3.56GB, +0.1149 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q4_K_M">
            <summary>
            K-Quant 4 bit (Medium)
            </summary>
            <remarks>Benchmark@7B: 3.80GB, +0.0535 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q5_K_S">
            <summary>
            K-Quant 5 bit (Small)
            </summary>
            <remarks>Benchmark@7B: 4.33GB, +0.0353 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q5_K_M">
            <summary>
            K-Quant 5 bit (Medium)
            </summary>
            <remarks>Benchmark@7B: 4.45GB, +0.0142 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q6_K">
            <summary>
            K-Quant 6 bit
            </summary>
            <remarks>Benchmark@7B: 5.15GB, +0.0044 ppl</remarks>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_IQ2_XXS">
            <summary>
            except 1d tensors
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_IQ2_XS">
            <summary>
            except 1d tensors
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q2_K_S">
            <summary>
            except 1d tensors
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_Q3_K_XS">
            <summary>
            except 1d tensors
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_MOSTLY_IQ3_XXS">
            <summary>
            except 1d tensors
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaFtype.LLAMA_FTYPE_GUESSED">
            <summary>
            File type was not specified
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaGrammarElementType">
            <summary>
            grammar element type
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElementType.END">
            <summary>
            end of rule definition
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElementType.ALT">
            <summary>
            start of alternate definition for rule
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElementType.RULE_REF">
            <summary>
            non-terminal element: reference to rule
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElementType.CHAR">
            <summary>
            terminal element: character (code point)
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElementType.CHAR_NOT">
            <summary>
            inverse char(s) ([^a], [^a-b] [^abc])
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElementType.CHAR_RNG_UPPER">
            <summary>
            modifies a preceding CHAR or CHAR_ALT to
            be an inclusive range ([a-z])
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElementType.CHAR_ALT">
            <summary>
            modifies a preceding CHAR or
            CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaGrammarElement">
            <summary>
            An element of a grammar
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElement.Type">
            <summary>
            The type of this element
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaGrammarElement.Value">
            <summary>
            Unicode code point or rule ID
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaGrammarElement.#ctor(LLama.Native.LLamaGrammarElementType,System.UInt32)">
            <summary>
            Construct a new LLamaGrammarElement
            </summary>
            <param name="type"></param>
            <param name="value"></param>
        </member>
        <member name="T:LLama.Native.LLamaKvCacheViewCell">
            <summary>
            Information associated with an individual cell in the KV cache view (llama_kv_cache_view_cell)
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaKvCacheViewCell.pos">
            <summary>
            The position for this cell. Takes KV cache shifts into account.
            May be negative if the cell is not populated.
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaKvCacheView">
            <summary>
            An updateable view of the KV cache (llama_kv_cache_view)
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaKvCacheViewSafeHandle">
            <summary>
            A safe handle for a LLamaKvCacheView
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaKvCacheViewSafeHandle.#ctor(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaKvCacheView)">
            <summary>
            Initialize a LLamaKvCacheViewSafeHandle which will call `llama_kv_cache_view_free` when disposed
            </summary>
            <param name="ctx"></param>
            <param name="view"></param>
        </member>
        <member name="M:LLama.Native.LLamaKvCacheViewSafeHandle.Allocate(LLama.Native.SafeLLamaContextHandle,System.Int32)">
            <summary>
            Allocate a new KV cache view which can be used to inspect the KV cache
            </summary>
            <param name="ctx"></param>
            <param name="maxSequences">The maximum number of sequences visible in this view per cell</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaKvCacheViewSafeHandle.ReleaseHandle">
            <inheritdoc />
        </member>
        <member name="M:LLama.Native.LLamaKvCacheViewSafeHandle.Update">
            <summary>
            Update this view
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaKvCacheViewSafeHandle.GetView">
            <summary>
            Get the raw KV cache view
            </summary>
            <returns></returns>
        </member>
        <member name="T:LLama.Native.NativeApi">
            <summary>
            Direct translation of the llama.cpp API
            </summary>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_view_init(LLama.Native.SafeLLamaContextHandle,System.Int32)">
            <summary>
            Create an empty KV cache view. (use only for debugging purposes)
            </summary>
            <param name="ctx"></param>
            <param name="n_max_seq"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_view_free(LLama.Native.LLamaKvCacheView@)">
            <summary>
            Free a KV cache view. (use only for debugging purposes)
            </summary>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_view_update(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaKvCacheView@)">
            <summary>
            Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)
            </summary>
            <param name="ctx"></param>
            <param name="view"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_get_kv_cache_token_count(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Returns the number of tokens in the KV cache (slow, use only for debug)
            If a KV cell has multiple sequences assigned to it, it will be counted multiple times
            </summary>
            <param name="ctx"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_get_kv_cache_used_cells(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Returns the number of used KV cells (i.e. have at least one sequence assigned to them)
            </summary>
            <param name="ctx"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.Native.NativeApi.LLamaBeamSearchCallback">
            <summary>
            Type of pointer to the beam_search_callback function.
            </summary>
            <param name="callback_data">callback_data is any custom data passed to llama_beam_search, that is subsequently passed back to beam_search_callbac</param>
            <param name="state"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_beam_search(LLama.Native.SafeLLamaContextHandle,LLama.Native.NativeApi.LLamaBeamSearchCallback,System.IntPtr,System.UInt64,System.Int32,System.Int32,System.Int32)">
            <summary>Deterministically returns entire sentence constructed by a beam search.</summary>
            <param name="ctx">Pointer to the llama_context.</param>
            <param name="callback">Invoked for each iteration of the beam_search loop, passing in beams_state.</param>
            <param name="callback_data">A pointer that is simply passed back to callback.</param>
            <param name="n_beams">Number of beams to use.</param>
            <param name="n_past">Number of tokens already evaluated.</param>
            <param name="n_predict">Maximum number of tokens to predict. EOS may occur earlier.</param>
            <param name="n_threads">Number of threads.</param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_empty_call">
            <summary>
            A method that does nothing. This is a native method, calling it will force the llama native dependencies to be loaded.
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_max_devices">
            <summary>
            Get the maximum number of devices supported by llama.cpp
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_model_default_params">
            <summary>
            Create a LLamaModelParams with default values
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_context_default_params">
            <summary>
            Create a LLamaContextParams with default values
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_model_quantize_default_params">
            <summary>
            Create a LLamaModelQuantizeParams with default values
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_supports_mmap">
            <summary>
            Check if memory mapping is supported
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_supports_mlock">
            <summary>
            Check if memory locking is supported
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_supports_gpu_offload">
            <summary>
            Check if GPU offload is supported
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_backend_init(System.Boolean)">
            <summary>
            Initialize the llama + ggml backend
            Call once at the start of the program
            </summary>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_set_rng_seed(LLama.Native.SafeLLamaContextHandle,System.UInt32)">
            <summary>
            Sets the current rng seed.
            </summary>
            <param name="ctx"></param>
            <param name="seed"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_get_state_size(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Returns the maximum size in bytes of the state (rng, logits, embedding
            and kv_cache) - will often be smaller after compacting tokens
            </summary>
            <param name="ctx"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_copy_state_data(LLama.Native.SafeLLamaContextHandle,System.Byte*)">
            <summary>
            Copies the state to the specified destination address.
            Destination needs to have allocated enough memory.
            </summary>
            <param name="ctx"></param>
            <param name="dest"></param>
            <returns>the number of bytes copied</returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_set_state_data(LLama.Native.SafeLLamaContextHandle,System.Byte*)">
            <summary>
            Set the state reading from the specified address
            </summary>
            <param name="ctx"></param>
            <param name="src"></param>
            <returns>the number of bytes read</returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_load_session_file(LLama.Native.SafeLLamaContextHandle,System.String,LLama.Native.LLamaToken[],System.UInt64,System.UInt64@)">
            <summary>
            Load session file
            </summary>
            <param name="ctx"></param>
            <param name="path_session"></param>
            <param name="tokens_out"></param>
            <param name="n_token_capacity"></param>
            <param name="n_token_count_out"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_save_session_file(LLama.Native.SafeLLamaContextHandle,System.String,LLama.Native.LLamaToken[],System.UInt64)">
            <summary>
            Save session file
            </summary>
            <param name="ctx"></param>
            <param name="path_session"></param>
            <param name="tokens"></param>
            <param name="n_token_count"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_eval(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaToken*,System.Int32,System.Int32)">
            <summary>
            Run the llama inference to obtain the logits and probabilities for the next token.
            tokens + n_tokens is the provided batch of new tokens to process
            n_past is the number of tokens to use from previous eval calls
            </summary>
            <param name="ctx"></param>
            <param name="tokens"></param>
            <param name="n_tokens"></param>
            <param name="n_past"></param>
            <returns>Returns 0 on success</returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_n_ctx(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Get the size of the context window for the model for this context
            </summary>
            <param name="ctx"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_get_logits(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Token logits obtained from the last call to llama_eval()
            The logits for the last token are stored in the last row
            Can be mutated in order to change the probabilities of the next token.<br />
            Rows: n_tokens<br />
            Cols: n_vocab
            </summary>
            <param name="ctx"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_get_logits_ith(LLama.Native.SafeLLamaContextHandle,System.Int32)">
            <summary>
            Logits for the ith token. Equivalent to: llama_get_logits(ctx) + i*n_vocab
            </summary>
            <param name="ctx"></param>
            <param name="i"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_get_embeddings(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Get the embeddings for the input
            </summary>
            <param name="ctx"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_bos(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the "Beginning of sentence" token
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_eos(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the "End of sentence" token
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_nl(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the "new line" token
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_add_bos_token(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Returns -1 if unknown, 1 for true or 0 for false.
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_add_eos_token(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Returns -1 if unknown, 1 for true or 0 for false.
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_prefix(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            codellama infill tokens, Beginning of infill prefix
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_middle(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            codellama infill tokens, Beginning of infill middle
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_suffix(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            codellama infill tokens, Beginning of infill suffix
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_eot(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            codellama infill tokens, End of infill middle
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_print_timings(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Print out timing information for this context
            </summary>
            <param name="ctx"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_reset_timings(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Reset all collected timing information for this context
            </summary>
            <param name="ctx"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_print_system_info">
            <summary>
            Print system information
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_token_to_piece(LLama.Native.SafeLlamaModelHandle,LLama.Native.LLamaToken,System.Span{System.Byte})">
            <summary>
            Convert a single token into text
            </summary>
            <param name="model"></param>
            <param name="llamaToken"></param>
            <param name="buffer">buffer to write string into</param>
            <returns>The length written, or if the buffer is too small a negative that indicates the length required</returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_tokenize(LLama.Native.SafeLlamaModelHandle,System.Byte*,System.Int32,LLama.Native.LLamaToken*,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            Convert text into tokens
            </summary>
            <param name="model"></param>
            <param name="text"></param>
            <param name="text_len"></param>
            <param name="tokens"></param>
            <param name="n_max_tokens"></param>
            <param name="add_bos"></param>
            <param name="special">Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext. Does not insert a leading space.</param>
            <returns>Returns the number of tokens on success, no more than n_max_tokens.
            Returns a negative number on failure - the number of tokens that would have been returned
            </returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_log_set(LLama.Native.LLamaLogCallback)">
            <summary>
            Register a callback to receive llama log messages
            </summary>
            <param name="logCallback"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_clear(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Clear the KV cache
            </summary>
            <param name="ctx"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_seq_rm(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos)">
            <summary>
            Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
            </summary>
            <param name="ctx"></param>
            <param name="seq"></param>
            <param name="p0"></param>
            <param name="p1"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_seq_cp(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaSeqId,LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos)">
            <summary>
            Copy all tokens that belong to the specified sequence to another sequence
            Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence
            </summary>
            <param name="ctx"></param>
            <param name="src"></param>
            <param name="dest"></param>
            <param name="p0"></param>
            <param name="p1"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_seq_keep(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaSeqId)">
            <summary>
            Removes all tokens that do not belong to the specified sequence
            </summary>
            <param name="ctx"></param>
            <param name="seq"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_seq_shift(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos,System.Int32)">
            <summary>
            Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
            If the KV cache is RoPEd, the KV data is updated accordingly
            </summary>
            <param name="ctx"></param>
            <param name="seq"></param>
            <param name="p0"></param>
            <param name="p1"></param>
            <param name="delta"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_kv_cache_seq_div(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos,System.Int32)">
            <summary>
            Integer division of the positions by factor of `d > 1`
            If the KV cache is RoPEd, the KV data is updated accordingly
            p0 &lt; 0 : [0,  p1]
            p1 &lt; 0 : [p0, inf)
            </summary>
            <param name="ctx"></param>
            <param name="seq"></param>
            <param name="p0"></param>
            <param name="p1"></param>
            <param name="d"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_batch_init(System.Int32,System.Int32,System.Int32)">
            <summary>
            Allocates a batch of tokens on the heap
            Each token can be assigned up to n_seq_max sequence ids
            The batch has to be freed with llama_batch_free()
            If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
            Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
            The rest of the llama_batch members are allocated with size n_tokens
            All members are left uninitialized
            </summary>
            <param name="n_tokens"></param>
            <param name="embd"></param>
            <param name="n_seq_max">Each token can be assigned up to n_seq_max sequence ids</param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_batch_free(LLama.Native.LLamaNativeBatch)">
            <summary>
            Frees a batch of tokens allocated with llama_batch_init()
            </summary>
            <param name="batch"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_decode(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaNativeBatch)">
            <summary>
            </summary>
            <param name="ctx"></param>
            <param name="batch"></param>
            <returns>Positive return values does not mean a fatal error, but rather a warning:<br />
             - 0: success<br />
             - 1: could not find a KV slot for the batch (try reducing the size of the batch or increase the context)<br />
             - &lt; 0: error<br />
            </returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_set_n_threads(LLama.Native.SafeLLamaContextHandle,System.UInt32,System.UInt32)">
            <summary>
            Set the number of threads used for decoding
            </summary>
            <param name="ctx"></param>
            <param name="n_threads">n_threads is the number of threads used for generation (single token)</param>
            <param name="n_threads_batch">n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_grammar_init(LLama.Native.LLamaGrammarElement**,System.UInt64,System.UInt64)">
            <summary>
            Create a new grammar from the given set of grammar rules
            </summary>
            <param name="rules"></param>
            <param name="n_rules"></param>
            <param name="start_rule_index"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_grammar_free(System.IntPtr)">
            <summary>
            Free all memory from the given SafeLLamaGrammarHandle
            </summary>
            <param name="grammar"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_grammar_copy(LLama.Native.SafeLLamaGrammarHandle)">
            <summary>
            Create a copy of an existing grammar instance
            </summary>
            <param name="grammar"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_grammar(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,LLama.Native.SafeLLamaGrammarHandle)">
            <summary>
            Apply constraints from grammar
            </summary>
            <param name="ctx"></param>
            <param name="candidates"></param>
            <param name="grammar"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_grammar_accept_token(LLama.Native.SafeLLamaContextHandle,LLama.Native.SafeLLamaGrammarHandle,LLama.Native.LLamaToken)">
            <summary>
            Accepts the sampled token into the grammar
            </summary>
            <param name="ctx"></param>
            <param name="grammar"></param>
            <param name="token"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.TryLoadLibrary">
            <summary>
            Try to load libllama, using CPU feature detection to try and load a more specialised DLL if possible
            </summary>
            <returns>The library handle to unload later, or IntPtr.Zero if no library was loaded</returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_model_quantize(System.String,System.String,LLama.Native.LLamaModelQuantizeParams*)">
            <summary>
            Returns 0 on success
            </summary>
            <param name="fname_inp"></param>
            <param name="fname_out"></param>
            <param name="param"></param>
            <returns>Returns 0 on success</returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_repetition_penalties(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,LLama.Native.LLamaToken*,System.UInt64,System.Single,System.Single,System.Single)">
            <summary>
            Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.
            Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="last_tokens"></param>
            <param name="last_tokens_size"></param>
            <param name="penalty_repeat">Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.</param>
            <param name="penalty_freq">Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</param>
            <param name="penalty_present">Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_apply_guidance(LLama.Native.SafeLLamaContextHandle,System.Single*,System.Single*,System.Single)">
            <summary>
            Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806
            </summary>
            <param name="ctx"></param>
            <param name="logits">Logits extracted from the original generation context.</param>
            <param name="logits_guidance">Logits extracted from a separate context from the same model.
            Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.</param>
            <param name="scale">Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.</param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_softmax(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@)">
            <summary>
            Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_top_k(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Int32,System.UInt64)">
            <summary>
            Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="k"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_top_p(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single,System.UInt64)">
            <summary>
            Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="p"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_min_p(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single,System.UInt64)">
            <summary>
            Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="p"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_tail_free(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single,System.UInt64)">
            <summary>
            Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="z"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_typical(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single,System.UInt64)">
            <summary>
            Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="p"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_typical(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single,System.Single,System.Single)">
            <summary>
            Dynamic temperature implementation described in the paper https://arxiv.org/abs/2309.02772.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="min_temp"></param>
            <param name="max_temp"></param>
            <param name="exponent_val"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_temp(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single)">
            <summary>
            Modify logits by temperature
            </summary>
            <param name="ctx"></param>
            <param name="candidates"></param>
            <param name="temp"></param>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_token_mirostat(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single,System.Single,System.Int32,System.Single@)">
            <summary>
            Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</param>
            <param name="tau">The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</param>
            <param name="eta">The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.</param>
            <param name="m">The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.</param>
            <param name="mu">Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_token_mirostat_v2(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@,System.Single,System.Single,System.Single@)">
            <summary>
            Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</param>
            <param name="tau">The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</param>
            <param name="eta">The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.</param>
            <param name="mu">Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_token_greedy(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@)">
            <summary>
            Selects the token with the highest probability.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.NativeApi.llama_sample_token(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArrayNative@)">
            <summary>
            Randomly selects a token from the candidates based on their probabilities.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <returns></returns>
        </member>
        <member name="T:LLama.Native.LLamaLogLevel">
            <summary>
            Severity level of a log message
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaLogLevel.Debug">
            <summary>
            Logs that are used for interactive investigation during development.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaLogLevel.Error">
            <summary>
            Logs that highlight when the current flow of execution is stopped due to a failure.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaLogLevel.Warning">
            <summary>
            Logs that highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaLogLevel.Info">
            <summary>
            Logs that track the general flow of the application.
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaModelMetadataOverride">
            <summary>
            Override a key/value pair in the llama model metadata (llama_model_kv_override)
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelMetadataOverride.key">
            <summary>
            Key to override
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelMetadataOverride.Tag">
            <summary>
            Type of value
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelMetadataOverride.PADDING">
            <summary>
            Add 4 bytes of padding, to align the next fields to 8 bytes
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelMetadataOverride.IntValue">
            <summary>
            Value, **must** only be used if Tag == LLAMA_KV_OVERRIDE_INT
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelMetadataOverride.FloatValue">
            <summary>
            Value, **must** only be used if Tag == LLAMA_KV_OVERRIDE_FLOAT
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelMetadataOverride.BoolValue">
            <summary>
            Value, **must** only be used if Tag == LLAMA_KV_OVERRIDE_BOOL
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaModelKvOverrideType">
            <summary>
            Specifies what type of value is being overridden by LLamaModelKvOverride
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelKvOverrideType.LLAMA_KV_OVERRIDE_INT">
            <summary>
            Overriding an int value
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelKvOverrideType.LLAMA_KV_OVERRIDE_FLOAT">
            <summary>
            Overriding a float value
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelKvOverrideType.LLAMA_KV_OVERRIDE_BOOL">
            <summary>
            Overriding a bool value
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaModelParams">
            <summary>
            A C# representation of the llama.cpp `llama_model_params` struct
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelParams.n_gpu_layers">
            <summary>
            // number of layers to store in VRAM
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelParams.split_mode">
            <summary>
            how to split the model across multiple GPUs
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelParams.main_gpu">
            <summary>
            the GPU that is used for scratch and small tensors
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelParams.tensor_split">
            <summary>
            how to split layers across multiple GPUs (size: <see cref="M:LLama.Native.NativeApi.llama_max_devices"/>)
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelParams.progress_callback">
            <summary>
            called with a progress value between 0 and 1, pass NULL to disable. If the provided progress_callback
            returns true, model loading continues. If it returns false, model loading is immediately aborted.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelParams.progress_callback_user_data">
            <summary>
            context pointer passed to the progress callback
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelParams.kv_overrides">
            <summary>
            override key-value pairs of the model meta data
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaModelParams.vocab_only">
            <summary>
            only load the vocabulary, no weights
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaModelParams.use_mmap">
            <summary>
            use mmap if possible
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaModelParams.use_mlock">
            <summary>
            force system to keep model in RAM
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaModelQuantizeParams">
            <summary>
            Quantizer parameters used in the native API
            </summary>
            <remarks>llama_model_quantize_params</remarks>
        </member>
        <member name="F:LLama.Native.LLamaModelQuantizeParams.nthread">
            <summary>
            number of threads to use for quantizing, if &lt;=0 will use std::thread::hardware_concurrency()
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelQuantizeParams.ftype">
            <summary>
            quantize to this llama_ftype
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaModelQuantizeParams.allow_requantize">
            <summary>
            allow quantizing non-f32/f16 tensors
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaModelQuantizeParams.quantize_output_tensor">
            <summary>
            quantize output.weight
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaModelQuantizeParams.only_copy">
            <summary>
            only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaModelQuantizeParams.pure">
            <summary>
            disable k-quant mixtures and quantize all tensors to the same type
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaModelQuantizeParams.imatrix">
            <summary>
            pointer to importance matrix data
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaNativeBatch">
            <summary>
            Input data for llama_decode
            A llama_batch object can contain input about one or many sequences
            The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaNativeBatch.n_tokens">
            <summary>
            The number of items pointed at by pos, seq_id and logits.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaNativeBatch.tokens">
            <summary>
            Either `n_tokens` of `llama_token`, or `NULL`, depending on how this batch was created
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaNativeBatch.embd">
            <summary>
            Either `n_tokens * embd * sizeof(float)` or `NULL`, depending on how this batch was created
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaNativeBatch.pos">
            <summary>
            the positions of the respective token in the sequence
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaNativeBatch.n_seq_id">
            <summary>
            https://github.com/ggerganov/llama.cpp/blob/master/llama.h#L139 ???
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaNativeBatch.seq_id">
            <summary>
            the sequence to which the respective token belongs
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaNativeBatch.logits">
            <summary>
            if zero, the logits for the respective token will not be output
            </summary>
        </member>
        <member name="T:LLama.Native.LLamaPos">
            <summary>
            Indicates position in a sequence
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaPos.Value">
            <summary>
            The raw value
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaPos.#ctor(System.Int32)">
            <summary>
            Create a new LLamaPos
            </summary>
            <param name="value"></param>
        </member>
        <member name="M:LLama.Native.LLamaPos.op_Explicit(LLama.Native.LLamaPos)~System.Int32">
            <summary>
            Convert a LLamaPos into an integer (extract the raw value)
            </summary>
            <param name="pos"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaPos.op_Implicit(System.Int32)~LLama.Native.LLamaPos">
            <summary>
            Convert an integer into a LLamaPos
            </summary>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaPos.op_Increment(LLama.Native.LLamaPos)">
            <summary>
            Increment this position
            </summary>
            <param name="pos"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaPos.op_Decrement(LLama.Native.LLamaPos)">
            <summary>
            Increment this position
            </summary>
            <param name="pos"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.Native.LLamaSeqId">
            <summary>
            ID for a sequence in a batch
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaSeqId.Zero">
            <summary>
            LLamaSeqId with value 0
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaSeqId.Value">
            <summary>
            The raw value
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaSeqId.#ctor(System.Int32)">
            <summary>
            Create a new LLamaSeqId 
            </summary>
            <param name="value"></param>
        </member>
        <member name="M:LLama.Native.LLamaSeqId.op_Explicit(LLama.Native.LLamaSeqId)~System.Int32">
            <summary>
            Convert a LLamaSeqId into an integer (extract the raw value)
            </summary>
            <param name="pos"></param>
        </member>
        <member name="M:LLama.Native.LLamaSeqId.op_Explicit(System.Int32)~LLama.Native.LLamaSeqId">
            <summary>
            Convert an integer into a LLamaSeqId
            </summary>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="T:LLama.Native.LLamaToken">
            <summary>
            A single token
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaToken.Value">
            <summary>
            The raw value
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaToken.#ctor(System.Int32)">
            <summary>
            Create a new LLamaToken
            </summary>
            <param name="value"></param>
        </member>
        <member name="M:LLama.Native.LLamaToken.op_Explicit(LLama.Native.LLamaToken)~System.Int32">
            <summary>
            Convert a LLamaToken into an integer (extract the raw value)
            </summary>
            <param name="pos"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaToken.op_Implicit(System.Int32)~LLama.Native.LLamaToken">
            <summary>
            Convert an integer into a LLamaToken
            </summary>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaToken.ToString">
            <inheritdoc />
        </member>
        <member name="T:LLama.Native.LLamaTokenData">
            <summary>
            A single token along with probability of this token being selected
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaTokenData.id">
            <summary>
            token id
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaTokenData.logit">
            <summary>
            log-odds of the token
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaTokenData.p">
            <summary>
            probability of the token
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaTokenData.#ctor(LLama.Native.LLamaToken,System.Single,System.Single)">
            <summary>
            Create a new LLamaTokenData
            </summary>
            <param name="id"></param>
            <param name="logit"></param>
            <param name="p"></param>
        </member>
        <member name="T:LLama.Native.LLamaTokenDataArray">
            <summary>
            Contains an array of LLamaTokenData, potentially sorted.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaTokenDataArray.data">
            <summary>
            The LLamaTokenData
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaTokenDataArray.sorted">
            <summary>
            Indicates if `data` is sorted by logits in descending order. If this is false the token data is in _no particular order_.
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.#ctor(System.Memory{LLama.Native.LLamaTokenData},System.Boolean)">
            <summary>
            Create a new LLamaTokenDataArray
            </summary>
            <param name="tokens"></param>
            <param name="isSorted"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.Create(System.ReadOnlySpan{System.Single})">
            <summary>
            Create a new LLamaTokenDataArray, copying the data from the given logits
            </summary>
            <param name="logits"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.OverwriteLogits(System.ReadOnlySpan{System.ValueTuple{LLama.Native.LLamaToken,System.Single}})">
            <summary>
            Overwrite the logit values for all given tokens
            </summary>
            <param name="values">tuples of token and logit value to overwrite</param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.ApplyGrammar(LLama.Native.SafeLLamaContextHandle,LLama.Native.SafeLLamaGrammarHandle)">
            <summary>
            Apply grammar rules to candidate tokens
            </summary>
            <param name="ctx"></param>
            <param name="grammar"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.TopK(LLama.Native.SafeLLamaContextHandle,System.Int32,System.UInt64)">
            <summary>
            Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
            </summary>
            <param name="context"></param>
            <param name="k">Number of tokens to keep</param>
            <param name="minKeep">Minimum number to keep</param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.TopP(LLama.Native.SafeLLamaContextHandle,System.Single,System.UInt64)">
            <summary>
            Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
            </summary>
            <param name="context"></param>
            <param name="p"></param>
            <param name="minKeep"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.MinP(LLama.Native.SafeLLamaContextHandle,System.Single,System.UInt64)">
            <summary>
            Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841
            </summary>
            <param name="context"></param>
            <param name="p">All tokens with probability greater than this will be kept</param>
            <param name="minKeep"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.TailFree(LLama.Native.SafeLLamaContextHandle,System.Single,System.UInt64)">
            <summary>
            Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.
            </summary>
            <param name="context"></param>
            <param name="z"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.LocallyTypical(LLama.Native.SafeLLamaContextHandle,System.Single,System.UInt64)">
            <summary>
            Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
            </summary>
            <param name="context"></param>
            <param name="p"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.RepetitionPenalty(LLama.Native.SafeLLamaContextHandle,System.ReadOnlySpan{LLama.Native.LLamaToken},System.Single,System.Single,System.Single)">
            <summary>
            Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.
            Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.
            </summary>
            <param name="context"></param>
            <param name="last_tokens"></param>
            <param name="penalty_repeat"></param>
            <param name="penalty_freq"></param>
            <param name="penalty_present"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.Temperature(LLama.Native.SafeLLamaContextHandle,System.Single)">
            <summary>
            Sample with temperature.
            As temperature increases, the prediction becomes more diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual
            </summary>
            <param name="context"></param>
            <param name="temp"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.Softmax(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
            </summary>
            <param name="context"></param>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.SampleToken(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Randomly selects a token from the candidates based on their probabilities.
            </summary>
            <param name="context"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.SampleTokenGreedy(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Selects the token with the highest probability.
            </summary>
            <param name="context"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.SampleTokenMirostat(LLama.Native.SafeLLamaContextHandle,System.Single,System.Single,System.Int32,System.Single@)">
            <summary>
            Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
            </summary>
            <param name="context"></param>
            <param name="tau">The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</param>
            <param name="eta">The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.</param>
            <param name="m">The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.</param>
            <param name="mu">Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArray.SampleTokenMirostat2(LLama.Native.SafeLLamaContextHandle,System.Single,System.Single,System.Single@)">
            <summary>
            Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
            </summary>
            <param name="context"></param>
            <param name="tau">The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</param>
            <param name="eta">The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.</param>
            <param name="mu">Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.</param>
            <returns></returns>
        </member>
        <member name="T:LLama.Native.LLamaTokenDataArrayNative">
            <summary>
            Contains a pointer to an array of LLamaTokenData which is pinned in memory.
            </summary>
        </member>
        <member name="F:LLama.Native.LLamaTokenDataArrayNative.data">
            <summary>
            A pointer to an array of LlamaTokenData
            </summary>
            <remarks>Memory must be pinned in place for all the time this LLamaTokenDataArrayNative is in use</remarks>
        </member>
        <member name="F:LLama.Native.LLamaTokenDataArrayNative.size">
            <summary>
            Number of LLamaTokenData in the array
            </summary>
        </member>
        <member name="P:LLama.Native.LLamaTokenDataArrayNative.sorted">
            <summary>
            Indicates if the items in the array are sorted
            </summary>
        </member>
        <member name="M:LLama.Native.LLamaTokenDataArrayNative.Create(LLama.Native.LLamaTokenDataArray,LLama.Native.LLamaTokenDataArrayNative@)">
            <summary>
            Create a new LLamaTokenDataArrayNative around the data in the LLamaTokenDataArray 
            </summary>
            <param name="array">Data source</param>
            <param name="native">Created native array</param>
            <returns>A memory handle, pinning the data in place until disposed</returns>
        </member>
        <member name="T:LLama.Native.LLamaLogCallback">
            <summary>
            Callback from llama.cpp with log messages
            </summary>
            <param name="level"></param>
            <param name="message"></param>
        </member>
        <member name="T:LLama.Native.RopeScalingType">
            <summary>
            RoPE scaling type. C# equivalent of llama_rope_scaling_type
            </summary>
        </member>
        <member name="T:LLama.Native.SafeLLamaContextHandle">
            <summary>
            A safe wrapper around a llama_context
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLLamaContextHandle.VocabCount">
            <summary>
            Total number of tokens in vocabulary of this model
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLLamaContextHandle.ContextSize">
            <summary>
            Total number of tokens in the context
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLLamaContextHandle.EmbeddingSize">
            <summary>
            Dimension of embedding vectors
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLLamaContextHandle.ModelHandle">
            <summary>
            Get the model which this context is using
            </summary>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.ReleaseHandle">
            <inheritdoc />
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.Create(LLama.Native.SafeLlamaModelHandle,LLama.Native.LLamaContextParams)">
            <summary>
            Create a new llama_state for the given model
            </summary>
            <param name="model"></param>
            <param name="lparams"></param>
            <returns></returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.llama_new_context_with_model(LLama.Native.SafeLlamaModelHandle,LLama.Native.LLamaContextParams)">
            <summary>
            Create a new llama_context with the given model. **This should never be called directly! Always use SafeLLamaContextHandle.Create**!
            </summary>
            <param name="model"></param>
            <param name="params"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.llama_free(System.IntPtr)">
            <summary>
            Frees all allocated memory in the given llama_context
            </summary>
            <param name="ctx"></param>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.GetLogits">
            <summary>
            Token logits obtained from the last call to llama_eval()
            The logits for the last token are stored in the last row
            Can be mutated in order to change the probabilities of the next token.<br />
            Rows: n_tokens<br />
            Cols: n_vocab
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.GetLogitsIth(System.Int32)">
            <summary>
            Logits for the ith token. Equivalent to: llama_get_logits(ctx) + i*n_vocab
            </summary>
            <param name="i"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.Tokenize(System.String,System.Boolean,System.Boolean,System.Text.Encoding)">
            <summary>
            Convert the given text into tokens
            </summary>
            <param name="text">The text to tokenize</param>
            <param name="add_bos">Whether the "BOS" token should be added</param>
            <param name="encoding">Encoding to use for the text</param>
            <param name="special">Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</param>
            <returns></returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.TokenToSpan(LLama.Native.LLamaToken,System.Span{System.Byte})">
            <summary>
            Convert a single llama token into bytes
            </summary>
            <param name="token">Token to decode</param>
            <param name="dest">A span to attempt to write into. If this is too small nothing will be written</param>
            <returns>The size of this token. **nothing will be written** if this is larger than `dest`</returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.Eval(System.ReadOnlySpan{LLama.Native.LLamaToken},System.Int32)">
            <summary>
            Run the llama inference to obtain the logits and probabilities for the next token.
            </summary>
            <param name="tokens">The provided batch of new tokens to process</param>
            <param name="n_past">the number of tokens to use from previous eval calls</param>
            <returns>Returns true on success</returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.Decode(LLama.Native.LLamaBatch)">
            <summary>
            </summary>
            <param name="batch"></param>
            <returns>Positive return values does not mean a fatal error, but rather a warning:<br />
             - 0: success<br />
             - 1: could not find a KV slot for the batch (try reducing the size of the batch or increase the context)<br />
             - &lt; 0: error<br />
            </returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.GetStateSize">
            <summary>
            Get the size of the state, when saved as bytes
            </summary>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.GetState(System.Byte*,System.UInt64)">
            <summary>
            Get the raw state of this context, encoded as bytes. Data is written into the `dest` pointer.
            </summary>
            <param name="dest">Destination to write to</param>
            <param name="size">Number of bytes available to write to in dest (check required size with `GetStateSize()`)</param>
            <returns>The number of bytes written to dest</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">Thrown if dest is too small</exception>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.GetState(System.IntPtr,System.UInt64)">
            <summary>
            Get the raw state of this context, encoded as bytes. Data is written into the `dest` pointer.
            </summary>
            <param name="dest">Destination to write to</param>
            <param name="size">Number of bytes available to write to in dest (check required size with `GetStateSize()`)</param>
            <returns>The number of bytes written to dest</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">Thrown if dest is too small</exception>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.SetState(System.Byte*)">
            <summary>
            Set the raw state of this context
            </summary>
            <param name="src">The pointer to read the state from</param>
            <returns>Number of bytes read from the src pointer</returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.SetState(System.IntPtr)">
            <summary>
            Set the raw state of this context
            </summary>
            <param name="src">The pointer to read the state from</param>
            <returns>Number of bytes read from the src pointer</returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.SetSeed(System.UInt32)">
            <summary>
            Set the RNG seed
            </summary>
            <param name="seed"></param>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.SetThreads(System.UInt32,System.UInt32)">
            <summary>
            Set the number of threads used for decoding
            </summary>
            <param name="threads">n_threads is the number of threads used for generation (single token)</param>
            <param name="threadsBatch">n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)</param>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheGetDebugView(System.Int32)">
            <summary>
            Get a new KV cache view that can be used to debug the KV cache
            </summary>
            <param name="maxSequences"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheCountCells">
            <summary>
            Count the number of used cells in the KV cache (i.e. have at least one sequence assigned to them)
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheCountTokens">
            <summary>
            Returns the number of tokens in the KV cache (slow, use only for debug)
            If a KV cell has multiple sequences assigned to it, it will be counted multiple times
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheClear">
            <summary>
            Clear the KV cache
            </summary>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheRemove(LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos)">
            <summary>
            Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
            </summary>
            <param name="seq"></param>
            <param name="p0"></param>
            <param name="p1"></param>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheSequenceCopy(LLama.Native.LLamaSeqId,LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos)">
            <summary>
            Copy all tokens that belong to the specified sequence to another sequence. Note that
            this does not allocate extra KV cache memory - it simply assigns the tokens to the
            new sequence
            </summary>
            <param name="src"></param>
            <param name="dest"></param>
            <param name="p0"></param>
            <param name="p1"></param>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheSequenceKeep(LLama.Native.LLamaSeqId)">
            <summary>
            Removes all tokens that do not belong to the specified sequence
            </summary>
            <param name="seq"></param>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheSequenceShift(LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos,System.Int32)">
            <summary>
            Adds relative position "delta" to all tokens that belong to the specified sequence
            and have positions in [p0, p1. If the KV cache is RoPEd, the KV data is updated
            accordingly
            </summary>
            <param name="seq"></param>
            <param name="p0"></param>
            <param name="p1"></param>
            <param name="delta"></param>
        </member>
        <member name="M:LLama.Native.SafeLLamaContextHandle.KvCacheSequenceDivide(LLama.Native.LLamaSeqId,LLama.Native.LLamaPos,LLama.Native.LLamaPos,System.Int32)">
            <summary>
            Integer division of the positions by factor of `d > 1`.
            If the KV cache is RoPEd, the KV data is updated accordingly.<br />
            p0 &lt; 0 : [0,  p1]<br />
            p1 &lt; 0 : [p0, inf)
            </summary>
            <param name="seq"></param>
            <param name="p0"></param>
            <param name="p1"></param>
            <param name="divisor"></param>
        </member>
        <member name="T:LLama.Native.SafeLLamaGrammarHandle">
            <summary>
            A safe reference to a `llama_grammar`
            </summary>
        </member>
        <member name="M:LLama.Native.SafeLLamaGrammarHandle.#ctor(System.IntPtr)">
            <summary>
            
            </summary>
            <param name="handle"></param>
        </member>
        <member name="M:LLama.Native.SafeLLamaGrammarHandle.ReleaseHandle">
            <inheritdoc />
        </member>
        <member name="M:LLama.Native.SafeLLamaGrammarHandle.Create(System.Collections.Generic.IReadOnlyList{LLama.Grammars.GrammarRule},System.UInt64)">
            <summary>
            Create a new llama_grammar
            </summary>
            <param name="rules">A list of list of elements, each inner list makes up one grammar rule</param>
            <param name="start_rule_index">The index (in the outer list) of the start rule</param>
            <returns></returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.Native.SafeLLamaGrammarHandle.Create(LLama.Native.LLamaGrammarElement**,System.UInt64,System.UInt64)">
            <summary>
            Create a new llama_grammar
            </summary>
            <param name="rules">rules list, each rule is a list of rule elements (terminated by a LLamaGrammarElementType.END element)</param>
            <param name="nrules">total number of rules</param>
            <param name="start_rule_index">index of the start rule of the grammar</param>
            <returns></returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.Native.SafeLLamaGrammarHandle.Clone">
            <summary>
            Create a copy of this grammar instance
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLLamaGrammarHandle.AcceptToken(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaToken)">
            <summary>
            Accepts the sampled token into the grammar
            </summary>
            <param name="ctx"></param>
            <param name="token"></param>
        </member>
        <member name="T:LLama.Native.SafeLLamaHandleBase">
            <summary>
            Base class for all llama handles to native resources
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLLamaHandleBase.IsInvalid">
            <inheritdoc />
        </member>
        <member name="M:LLama.Native.SafeLLamaHandleBase.ToString">
            <inheritdoc />
        </member>
        <member name="T:LLama.Native.SafeLlamaModelHandle">
            <summary>
            A reference to a set of llama model weights
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.VocabCount">
            <summary>
            Total number of tokens in vocabulary of this model
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.ContextSize">
            <summary>
            Total number of tokens in the context
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.RopeFrequency">
            <summary>
            Get the rope frequency this model was trained with
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.EmbeddingSize">
            <summary>
            Dimension of embedding vectors
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.SizeInBytes">
            <summary>
            Get the size of this model in bytes
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.ParameterCount">
            <summary>
            Get the number of parameters in this model
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.Description">
            <summary>
            Get a description of this model
            </summary>
        </member>
        <member name="P:LLama.Native.SafeLlamaModelHandle.MetadataCount">
            <summary>
            Get the number of metadata key/value pairs
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.ReleaseHandle">
            <inheritdoc />
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.LoadFromFile(System.String,LLama.Native.LLamaModelParams)">
            <summary>
            Load a model from the given file path into memory
            </summary>
            <param name="modelPath"></param>
            <param name="lparams"></param>
            <returns></returns>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_load_model_from_file(System.String,LLama.Native.LLamaModelParams)">
            <summary>
            Load all of the weights of a model into memory.
            </summary>
            <param name="path_model"></param>
            <param name="params"></param>
            <returns>The loaded model, or null on failure.</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_apply_lora_from_file(LLama.Native.SafeLlamaModelHandle,System.String,System.Single,System.String,System.Int32)">
            <summary>
            Apply a LoRA adapter to a loaded model
            path_base_model is the path to a higher quality model to use as a base for
            the layers modified by the adapter. Can be NULL to use the current loaded model.
            The model needs to be reloaded before applying a new adapter, otherwise the adapter
            will be applied on top of the previous one
            </summary>
            <param name="model_ptr"></param>
            <param name="path_lora"></param>
            <param name="scale"></param>
            <param name="path_base_model"></param>
            <param name="n_threads"></param>
            <returns>Returns 0 on success</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_free_model(System.IntPtr)">
            <summary>
            Frees all allocated memory associated with a model
            </summary>
            <param name="model"></param>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_meta_count(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the number of metadata key/value pairs
            </summary>
            <param name="model"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_meta_key_by_index(LLama.Native.SafeLlamaModelHandle,System.Int32,System.Span{System.Byte})">
            <summary>
            Get metadata key name by index
            </summary>
            <param name="model">Model to fetch from</param>
            <param name="index">Index of key to fetch</param>
            <param name="dest">buffer to write result into</param>
            <returns>The length of the string on success (even if the buffer is too small). -1 is the key does not exist.</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_meta_val_str_by_index(LLama.Native.SafeLlamaModelHandle,System.Int32,System.Span{System.Byte})">
            <summary>
            Get metadata value as a string by index
            </summary>
            <param name="model">Model to fetch from</param>
            <param name="index">Index of val to fetch</param>
            <param name="dest">Buffer to write result into</param>
            <returns>The length of the string on success (even if the buffer is too small). -1 is the key does not exist.</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_meta_val_str(LLama.Native.SafeLlamaModelHandle,System.Byte*,System.Byte*,System.Int64)">
            <summary>
            Get metadata value as a string by key name
            </summary>
            <param name="model"></param>
            <param name="key"></param>
            <param name="buf"></param>
            <param name="buf_size"></param>
            <returns>The length of the string on success, or -1 on failure</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_n_vocab(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the number of tokens in the model vocabulary
            </summary>
            <param name="model"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_n_ctx_train(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the size of the context window for the model
            </summary>
            <param name="model"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_n_embd(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the dimension of embedding vectors from this model
            </summary>
            <param name="model"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_desc(LLama.Native.SafeLlamaModelHandle,System.Byte*,System.Int64)">
            <summary>
            Get a string describing the model type
            </summary>
            <param name="model"></param>
            <param name="buf"></param>
            <param name="buf_size"></param>
            <returns>The length of the string on success (even if the buffer is too small)., or -1 on failure</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_size(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the size of the model in bytes
            </summary>
            <param name="model"></param>
            <returns>The size of the model</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_model_n_params(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the number of parameters in this model
            </summary>
            <param name="model"></param>
            <returns>The functions return the length of the string on success, or -1 on failure</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.llama_rope_freq_scale_train(LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Get the model's RoPE frequency scaling factor
            </summary>
            <param name="model"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.ApplyLoraFromFile(System.String,System.Single,System.String,System.Nullable{System.Int32})">
            <summary>
            Apply a LoRA adapter to a loaded model
            </summary>
            <param name="lora"></param>
            <param name="scale"></param>
            <param name="modelBase">A path to a higher quality model to use as a base for the layers modified by the
            adapter. Can be NULL to use the current loaded model.</param>
            <param name="threads"></param>
            <exception cref="T:LLama.Exceptions.RuntimeError"></exception>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.TokenToSpan(LLama.Native.LLamaToken,System.Span{System.Byte})">
            <summary>
            Convert a single llama token into bytes
            </summary>
            <param name="token">Token to decode</param>
            <param name="dest">A span to attempt to write into. If this is too small nothing will be written</param>
            <returns>The size of this token. **nothing will be written** if this is larger than `dest`</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.TokensToSpan(System.Collections.Generic.IReadOnlyList{LLama.Native.LLamaToken},System.Span{System.Char},System.Text.Encoding)">
            <summary>
            Convert a sequence of tokens into characters.
            </summary>
            <param name="tokens"></param>
            <param name="dest"></param>
            <param name="encoding"></param>
            <returns>The section of the span which has valid data in it.
            If there was insufficient space in the output span this will be
            filled with as many characters as possible, starting from the _last_ token.
            </returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.Tokenize(System.String,System.Boolean,System.Boolean,System.Text.Encoding)">
            <summary>
            Convert a string of text into tokens
            </summary>
            <param name="text"></param>
            <param name="add_bos"></param>
            <param name="encoding"></param>
            <param name="special">Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.CreateContext(LLama.Native.LLamaContextParams)">
            <summary>
            Create a new context for this model
            </summary>
            <param name="params"></param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.MetadataKeyByIndex(System.Int32)">
            <summary>
            Get the metadata key for the given index
            </summary>
            <param name="index">The index to get</param>
            <returns>The key, null if there is no such key or if the buffer was too small</returns>
        </member>
        <member name="M:LLama.Native.SafeLlamaModelHandle.MetadataValueByIndex(System.Int32)">
            <summary>
            Get the metadata value for the given index
            </summary>
            <param name="index">The index to get</param>
            <returns>The value, null if there is no such value or if the buffer was too small</returns>
        </member>
        <member name="T:LLama.Native.SamplingApi">
            <summary>
            Direct translation of the llama.cpp sampling API
            </summary>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_grammar(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,LLama.Native.SafeLLamaGrammarHandle)">
            <summary>
            Apply grammar rules to candidate tokens
            </summary>
            <param name="ctx"></param>
            <param name="candidates"></param>
            <param name="grammar"></param>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_softmax(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray)">
            <summary>
            Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_top_k(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.Int32,System.UInt64)">
            <summary>
            Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="k"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_top_p(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.Single,System.UInt64)">
            <summary>
            Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="p"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_tail_free(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.Single,System.UInt64)">
            <summary>
            Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="z"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_typical(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.Single,System.UInt64)">
            <summary>
            Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <param name="p"></param>
            <param name="min_keep"></param>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_temperature(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.Single)">
            <summary>
            Sample with temperature.
            As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual
            </summary>
            <param name="ctx"></param>
            <param name="candidates"></param>
            <param name="temp"></param>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_token_mirostat(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.Single,System.Single,System.Int32,System.Single@)">
            <summary>
            Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">A vector of `LLamaTokenData` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</param>
            <param name="tau">The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</param>
            <param name="eta">The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.</param>
            <param name="m">The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.</param>
            <param name="mu">Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_token_mirostat_v2(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.Single,System.Single,System.Single@)">
            <summary>
            Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">A vector of `LLamaTokenData` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</param>
            <param name="tau">The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</param>
            <param name="eta">The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.</param>
            <param name="mu">Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_token_greedy(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray)">
            <summary>
            Selects the token with the highest probability.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Native.SamplingApi.llama_sample_token(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray)">
            <summary>
            Randomly selects a token from the candidates based on their probabilities.
            </summary>
            <param name="ctx"></param>
            <param name="candidates">Pointer to LLamaTokenDataArray</param>
            <returns></returns>
        </member>
        <member name="T:LLama.Sampling.BaseSamplingPipeline">
            <summary>
            Base class for implementing custom sampling pipelines. This provides a helpful framework for implementing `ISamplingPipeline`.
            </summary>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.Sample(LLama.Native.SafeLLamaContextHandle,System.Span{System.Single},System.ReadOnlySpan{LLama.Native.LLamaToken})">
            <inheritdoc/>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.Accept(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaToken)">
            <inheritdoc />
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.GetProtectedTokens(LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Get all of the "protected" tokens that cannot be changed by ProcessLogits
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.RestoreProtectedTokens(System.Span{System.Single})">
            <summary>
            Restore the value of the "protected" tokens which were saved before the call to ProcessLogits
            </summary>
            <param name="logits"></param>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.RestoreProtectedTokens(LLama.Native.LLamaTokenDataArray)">
            <summary>
            Restore the value of the "protected" tokens which were saved before the call to ProcessLogits
            </summary>
            <param name="candidates"></param>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.ProcessLogits(LLama.Native.SafeLLamaContextHandle,System.Span{System.Single},System.ReadOnlySpan{LLama.Native.LLamaToken})">
            <summary>
            Process the raw logit values
            </summary>
            <param name="ctx">The context being sampled from</param>
            <param name="logits">The logits produced by the model</param>
            <param name="lastTokens">A list of tokens recently returned by the model</param>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.ProcessTokenDataArray(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.ReadOnlySpan{LLama.Native.LLamaToken})">
            <summary>
            Process the LLamaTokenDataArray and select a single token
            </summary>
            <param name="ctx">The context being sampled from</param>
            <param name="candidates">The LLamaTokenDataArray data produced by the model</param>
            <param name="lastTokens">A list of tokens recently returned by the model</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.Reset">
            <inheritdoc/>
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.Clone">
            <inheritdoc />
        </member>
        <member name="M:LLama.Sampling.BaseSamplingPipeline.Dispose">
            <inheritdoc/>
        </member>
        <member name="T:LLama.Sampling.DefaultSamplingPipeline">
            <summary>
            An implementation of ISamplePipeline which mimics the default llama.cpp sampling
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.LogitBias">
            <summary>
            Bias values to add to certain logits
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.Grammar">
            <summary>
            Grammar to constrain valid tokens
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.RepeatPenalty">
            <summary>
            Repetition penalty, as described in https://arxiv.org/abs/1909.05858
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.AlphaFrequency">
            <summary>
            Frequency penalty as described by OpenAI: https://platform.openai.com/docs/api-reference/chat/create<br />
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text
            so far, decreasing the model's likelihood to repeat the same line verbatim.
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.AlphaPresence">
            <summary>
            Presence penalty as described by OpenAI: https://platform.openai.com/docs/api-reference/chat/create<br />
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the
            text so far, increasing the model's likelihood to talk about new topics.
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.Temperature">
            <summary>
            Temperature to apply (higher temperature is more "creative")
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.TopK">
            <summary>
            Number of tokens to keep in TopK sampling
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.TailFreeZ">
            <summary>
            Z value for tail free sampling
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.TypicalP">
            <summary>
            P value for locally typical sampling
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.TopP">
            <summary>
            P value for TopP sampling
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.MinP">
            <summary>
            P value for MinP sampling
            </summary>
        </member>
        <member name="P:LLama.Sampling.DefaultSamplingPipeline.PenalizeNewline">
            <summary>
            Whether the newline value should be protected from being modified by logit bias and repeat penalty
            </summary>
        </member>
        <member name="M:LLama.Sampling.DefaultSamplingPipeline.GetProtectedTokens(LLama.Native.SafeLLamaContextHandle)">
            <inheritdoc />
        </member>
        <member name="M:LLama.Sampling.DefaultSamplingPipeline.ProcessLogits(LLama.Native.SafeLLamaContextHandle,System.Span{System.Single},System.ReadOnlySpan{LLama.Native.LLamaToken})">
            <inheritdoc />
        </member>
        <member name="M:LLama.Sampling.DefaultSamplingPipeline.ProcessTokenDataArray(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaTokenDataArray,System.ReadOnlySpan{LLama.Native.LLamaToken})">
            <inheritdoc />
        </member>
        <member name="M:LLama.Sampling.DefaultSamplingPipeline.Clone">
            <inheritdoc />
        </member>
        <member name="T:LLama.Sampling.ISamplingPipeline">
            <summary>
            Convert a span of logits into a single sampled token. This interface can be implemented to completely customise the sampling process.
            </summary>
        </member>
        <member name="M:LLama.Sampling.ISamplingPipeline.Sample(LLama.Native.SafeLLamaContextHandle,System.Span{System.Single},System.ReadOnlySpan{LLama.Native.LLamaToken})">
            <summary>
            Sample a single token from the given logits
            </summary>
            <param name="ctx">The context being sampled from</param>
            <param name="logits">The logits produced by the model</param>
            <param name="lastTokens">A span of tokens recently returned by the model</param>
            <returns></returns>
        </member>
        <member name="M:LLama.Sampling.ISamplingPipeline.Accept(LLama.Native.SafeLLamaContextHandle,LLama.Native.LLamaToken)">
            <summary>
            Update the pipeline, with knowledge that a particular token was just accepted
            </summary>
            <param name="ctx"></param>
            <param name="token"></param>
        </member>
        <member name="M:LLama.Sampling.ISamplingPipeline.Reset">
            <summary>
            Reset all internal state of the sampling pipeline
            </summary>
        </member>
        <member name="M:LLama.Sampling.ISamplingPipeline.Clone">
            <summary>
            Create a copy of this sampling pipeline
            </summary>
            <returns></returns>
        </member>
        <member name="T:LLama.Sampling.ISamplingPipelineExtensions">
            <summary>
            Extensions methods for ISamplingPipeline
            </summary>
        </member>
        <member name="M:LLama.Sampling.ISamplingPipelineExtensions.Sample(LLama.Sampling.ISamplingPipeline,LLama.Native.SafeLLamaContextHandle,System.Span{System.Single},System.Collections.Generic.List{LLama.Native.LLamaToken})">
            <summary>
            Sample a single token from the given logits
            </summary>
            <param name="pipeline"></param>
            <param name="ctx">The context being sampled from</param>
            <param name="logits">The logits produced by the model</param>
            <param name="lastTokens">A list of tokens recently returned by the model</param>
            <returns></returns>
        </member>
        <member name="T:LLama.StreamingTokenDecoder">
            <summary>
            Decodes a stream of tokens into a stream of characters
            </summary>
        </member>
        <member name="P:LLama.StreamingTokenDecoder.AvailableCharacters">
            <summary>
            The number of decoded characters waiting to be read
            </summary>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.#ctor(System.Text.Encoding,LLama.LLamaWeights)">
            <summary>
            Create a new decoder
            </summary>
            <param name="encoding">Text encoding to use</param>
            <param name="weights">Model weights</param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.#ctor(LLama.LLamaContext)">
            <summary>
            Create a new decoder
            </summary>
            <param name="context">Context to retrieve encoding and model weights from</param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.#ctor(System.Text.Encoding,LLama.Native.SafeLLamaContextHandle)">
            <summary>
            Create a new decoder
            </summary>
            <param name="encoding">Text encoding to use</param>
            <param name="context">Context to retrieve model weights from</param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.#ctor(System.Text.Encoding,LLama.Native.SafeLlamaModelHandle)">
            <summary>
            Create a new decoder
            </summary>
            <param name="encoding">Text encoding to use</param>
            <param name="weights">Models weights to use</param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.Add(LLama.Native.LLamaToken)">
            <summary>
            Add a single token to the decoder
            </summary>
            <param name="token"></param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.Add(System.Int32)">
            <summary>
            Add a single token to the decoder
            </summary>
            <param name="token"></param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.AddRange(System.Collections.Generic.IEnumerable{System.Int32})">
            <summary>
            Add all tokens in the given enumerable
            </summary>
            <param name="tokens"></param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.AddRange(System.Collections.Generic.IEnumerable{LLama.Native.LLamaToken})">
            <summary>
            Add all tokens in the given enumerable
            </summary>
            <param name="tokens"></param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.Read(System.Collections.Generic.List{System.Char})">
            <summary>
            Read all decoded characters and clear the buffer
            </summary>
            <param name="dest"></param>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.Read">
            <summary>
            Read all decoded characters as a string and clear the buffer
            </summary>
            <returns></returns>
        </member>
        <member name="M:LLama.StreamingTokenDecoder.Reset">
            <summary>
            Set the decoder back to its initial state
            </summary>
        </member>
    </members>
</doc>
